# -*- coding: utf-8 -*-
"""06-test-characteristic-of-conformity-filtered-classes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UOuTClmxv1aDNYs8IAXU4V9knwWFnLLc

# Testing conformity characteristic
    
State-of-the-art says that conformal learning gives conformity guarantees, that is we can bound the error rate with a predifined value. In this notebook we test this property of conformal prediction and also check if the same properties can be achieved in a different way.
    
We follow experimental setup from 'Model-Agnostic Nonconformity Functions for Conformal Classification' (check `papers` folder in repository):
1. 10-fold cross validation (90\\% - training data, 10\\% - test data)
2. Training data is split into proper training and calibration set in the following proportion: 4:1 (80\\% - proper training, 20\\% calibration)
3. It means we have:
    1. 72\\% - proper training
    2. 18\\% - calibrations
    3. 10\\% - testing
        
**TODO:**
1. Run conformal classification with SVM for 3 datasets `breast cancer`, `iris`, `wine` with $\epsilon \in \{0.01, 0.05, 0.1, 0.2\}$ and check what will be the error rate. This is similar to results in Tab. V from the paper (a value for every dataset: mean + std)
2. Check if we can get similar results in a different way

Mounting shared folder
"""

path_to_files = '/home/faisal/Desktop/Master-thesis-Jehan-20201119T121738Z-001/Master-thesis-Jehan'
"""Installing libraries"""

#!pip install nonconformist

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import copy

from nonconformist.cp import IcpClassifier
from nonconformist.nc import NcFactory
from nonconformist.nc import InverseProbabilityErrFunc
from nonconformist.nc import MarginErrFunc

from sklearn.datasets import load_breast_cancer
from sklearn.datasets import load_iris

from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier

from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis

# import warnings filter
from warnings import simplefilter
# ignore all future warnings
simplefilter(action='ignore', category=FutureWarning)
simplefilter(action='ignore', category=UserWarning)

"""# Functions definition

## Analysis
"""

def get_fold(fold, idx, test_len, cal_len):
    idx = np.array(idx)
    start_idx = (fold-1) * test_len
    end_idx = min(fold * test_len, len(idx))
    idx_test = idx[start_idx:end_idx]
    idx_train = idx[0:start_idx].tolist()
    idx_train.extend(idx[end_idx:].tolist())
    # todo the code will not compile, as idx_all_train is unknown
    # in idx_train you have proper training and calibration sets, you now have to split them into 2 separate sets
    # we agreed to do it like this
    # take first cal_len elements from idx_all_train as calibration and the rest as proper train set
    idx_cal = idx_train[0:cal_len]
    idx_train = idx_train[cal_len:]
    return np.array(idx_train), np.array(idx_test), np.array(idx_cal)


# get size of training, calibration and testing sets
def get_train_cal_test_len(N, test_frac=0.1, cal_frac=0.2):
    # N - size of the dataset
    # test_frac - fraction of test data from the whole dataset, we'll use test_frac=0.1
    # test_frac depends on the number of folds, test_frac=1/num_folds
    # whole dataset = test_data + all_training_data
    # cal_frac - fraction of calibration data from the all_training_data, we'll use cal_frac=0.2
    # all_training_data = calibration_data + training_data
    test_len = round(N * test_frac)
    all_train_len = N - test_len
    cal_len = round(all_train_len * cal_frac)
    train_len = all_train_len - cal_len
    return(train_len, cal_len, test_len)


# calculating metrics: oneC & avgC
def get_oneC_avgC(prediction):
    arr = np.array(prediction)
    oneC = 0
    avgC = 0
    for i in range(0, len(arr)):
        num_predicted = arr[i].sum()
        avgC += num_predicted
        if num_predicted == 1:
            oneC += 1
        pass
    oneC /= len(arr)
    avgC /= len(arr)
    return (oneC,avgC)


# calculating metrics: error_rate
def get_accuracy(prediction, real_class):
    correct = 0
    N = len(prediction)
    for i in range(0, N):
        if (real_class[i] < len(prediction[i])) and (prediction[i][real_class[i]]):
            correct += 1
        pass
    return correct / N


def filter_class_res(prediction, epsilon):
    result = []
    for line in prediction:
        tmp = copy.deepcopy(line)
        removed = 0
        # removed_idx = []

        to_continue = True
        while to_continue:
            min_idx = np.argmin(tmp)
            if tmp[min_idx] + removed <= epsilon:
                removed += tmp[min_idx]
                # removed_idx.append(min_idx)
                tmp[min_idx] = 2.
            else:
                to_continue = False
                pass
            pass
        # now set remove all values set to 2.
        result.append(tmp < 2)
        # print(line)
        pass
    return np.array(result)


def filter_class_res_total(prediction, epsilon):
    tmp = copy.deepcopy(prediction).flatten()
    tot_epsilon = epsilon * len(prediction)
    removed = 0
    to_continue = True
    while to_continue:
        min_idx = np.argmin(tmp)
        if tmp[min_idx] + removed < tot_epsilon:
            removed += tmp[min_idx]
            tmp[min_idx] = 2.
            pass
        else:
            to_continue = False
            pass
        pass
    result = tmp < 2.
    result = result.reshape(prediction.shape)
    return result

"""Functions for plotting"""

def autolabel(rects):
    #Attach a text label above each bar in *rects*, displaying its height.
    for rect in rects:
        height = rect.get_height()
        ax.annotate('{}'.format(height),
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 1),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')


def plot_2_df(df_1, df_2, columns, title_str, to_extend=None):
  # model 1
  df = df_1
  color = 'blue'

  if columns[0] is not None:
    plt.plot(eps_err, df[columns[0]], label=df_1.name, color=color, lw=3) # thick line for original 
    plt.plot(eps_err, df[columns[1]], label='- margin', color=color, ) # normal line for margin 
    plt.plot(eps_err, df[columns[2]], label='- inv prob', color=color, linestyle='--') # dashed line for inv_prob  
    plt.plot(eps_err, df[columns[3]], label='- simulated', color=color, linestyle=':') # dotted line for simulation 
  else:
    plt.plot(eps_err, df[columns[1]], label=df_1.name + '- margin', color=color, ) # normal line for margin error
    plt.plot(eps_err, df[columns[2]], label='- inv prob', color=color, linestyle='--') # dashed line for inv_prob error
    plt.plot(eps_err, df[columns[3]], label='- simulated', color=color, linestyle=':') # dotted line for simulation error


  # model 2
  df = df_2
  color = 'orange'

  if columns[0] is not None:
    plt.plot(eps_err, df[columns[0]], label=df_2.name, color=color, lw=3) # thik line for original 
    plt.plot(eps_err, df[columns[1]], label='_nolegend_', color=color, ) # normal line for margin 
    plt.plot(eps_err, df[columns[2]], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob 
    plt.plot(eps_err, df[columns[3]], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation 
  else:
    plt.plot(eps_err, df[columns[1]], label=df_2.name, color=color, ) # normal line for margin error
    plt.plot(eps_err, df[columns[2]], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
    plt.plot(eps_err, df[columns[3]], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

  plt.xticks(eps_err)
  if columns[0] is not None:
    y_ticks = eps_err[:]
    if to_extend is not None:
      y_ticks.extend(to_extend)
    plt.yticks(y_ticks)
  plt.grid(True)
  plt.legend()
  plt.title(title_str)
  plt.xlabel('Error, $\epsilon$')
  plt.ylabel(title_str)
  pass

"""## Data loading"""

def load_wine_red():
    print('Loading wine_red')
    # load red wine dataset
    file = path_to_files + 'datasets/winequality-red.csv'
    df = pd.read_csv(file, sep=';')
    target_names = df[df.columns[-1]].unique()
    target_names.sort()
    target = df[df.columns[-1]].values
    target -= min(target)
    data = {'target': df[df.columns[-1]].values, 'data': df[df.columns[:-1]].values,
            'target_names': target_names}
    return data

def load_wine_white():
    print('Loading wine_white')
    # load red wine dataset
    file = path_to_files + 'datasets/winequality-white.csv'
    df = pd.read_csv(file, sep=';')
    target_names = df[df.columns[-1]].unique()
    target_names.sort()
    target = df[df.columns[-1]].values
    target -= min(target)
    data = {'target': df[df.columns[-1]].values, 'data': df[df.columns[:-1]].values,
            'target_names': target_names}
    return data

"""## This is the main function, runs conformal prediction"""

def run_conformal(data, epsilon_arr, model_obj=SVC, params={'probability': True},
                  err_func=MarginErrFunc, smoothing=True, seed=1, verbose=True):
    np.random.seed(seed=seed)
    
    data_size = len(data['target'])
    idx = np.random.permutation(data_size)
    train_len, cal_len, test_len = get_train_cal_test_len(data_size, 0.1, 0.2)

    if verbose:
        print('Info about dataset: ')
        print('classes: {}'.format(list(data['target_names'])))
        print('size of the dataset: {}'.format(len(data['target'])))
        print('train = {}, cal = {}, test = {}'.format(train_len, cal_len, test_len))
        pass
    
    # define arrays to store information in
    res_epsilon_arr = []
    fold_arr = []
    # error rate for normal SVM
    error_rate_trad_arr = []
    # info for conformal learning 
    oneC_arr = []
    avgC_arr = []
    error_rate_arr = []
    # info for simulation of conformal learning (exclude labels until you get 1-eps total probability)
    oneC_svm_arr = []
    avgC_svm_arr = []
    error_rate_svm_arr = []
    
    for fold in range(1, num_folds + 1):
        idx_train, idx_test, idx_cal = get_fold(fold, idx, test_len, cal_len)
        # first run normal SVM
        # put train and calibration together
        idx_all_train = np.concatenate((idx_train, idx_cal))
        clf = model_obj(**params)
        clf.fit(data['data'][idx_all_train, :], data['target'][idx_all_train])
        prediction = clf.predict(data['data'][idx_test, :])
        error_rate_svm = (data['target'][idx_test] != prediction).sum() / len(prediction)
        
        # now for different values of eps do conformal prediction

        # build conformal model
        model = model_obj(**params)  # Create the underlying model
        nc = NcFactory.create_nc(model, err_func=err_func())  # specify non-conformity function
        icp = IcpClassifier(nc, smoothing=smoothing)
        # Fit the ICP using the proper training set
        icp.fit(data['data'][idx_train, :], data['target'][idx_train])
        # Calibrate the ICP using the calibration set
        icp.calibrate(data['data'][idx_cal, :], data['target'][idx_cal])

        for i in range(0, len(epsilon_arr)):
            epsilon = epsilon_arr[i]
            # save current info to arrays
            fold_arr.append(fold)
            res_epsilon_arr.append(epsilon)
            # save info about traditional SVM error rate
            error_rate_trad_arr.append(error_rate_svm)
            # run adjusted conformal prediction: (exclude labels until you get 1-eps total probability)
            # see function filter_class_res
            prediction_svm_prob = clf.predict_proba(data['data'][idx_test, :])
            prediction_svm = filter_class_res(prediction_svm_prob, epsilon)
            oneC, avgC = get_oneC_avgC(prediction_svm)
            real_class = data['target'][idx_test]
            accuracy = get_accuracy(prediction_svm, real_class)
            error_rate = 1 - accuracy
            oneC_svm_arr.append(oneC)
            avgC_svm_arr.append(avgC)
            error_rate_svm_arr.append(error_rate)
            # run conformal prediction itself
            # Produce predictions for the test set
            prediction = icp.predict(data['data'][idx_test, :], significance=epsilon)
            oneC, avgC = get_oneC_avgC(prediction)
            accuracy = get_accuracy(prediction, real_class)
            error_rate = 1 - accuracy
            oneC_arr.append(oneC)
            avgC_arr.append(avgC)
            error_rate_arr.append(error_rate)
            pass
        pass
    
    df_res = pd.DataFrame({})
    # save data into a data frame
    df_res['eps'] = res_epsilon_arr
    df_res['fold'] = fold_arr
    df_res['origin_err'] = error_rate_trad_arr
    # info for conformal learning 
    df_res['Conf_err'] = error_rate_arr
    df_res['Conf_oneC'] = oneC_arr
    df_res['Conf_avgC'] = avgC_arr
    # info for simulation of conformal learning
    df_res['Simul_err'] = error_rate_svm_arr
    df_res['Simul_oneC'] = oneC_svm_arr
    df_res['Simul_avgC'] = avgC_svm_arr
    # reorder
    df_res.sort_values(by=['eps', 'fold'], inplace=True)
    #df_res.reset_index(inplace=True)
    
    return df_res

"""And analysis of conformal prediction with 2 error functions"""

def analysis_with_2_func(data, model_obj, params, epsilon_arr, smoothing=True, seed=1, sub_verbose=True):
  margin_res_df = run_conformal(data=data, model_obj=model_obj, params=params, 
                                epsilon_arr=epsilon_arr, err_func=MarginErrFunc, 
                                smoothing=smoothing, seed=seed, verbose=sub_verbose)
  model_mean_df = margin_res_df.groupby('eps').mean()
  # now inverse probability error func


  inverse_res_df = run_conformal(data=data, model_obj=model_obj, params=params, 
                                epsilon_arr=epsilon_arr, err_func=InverseProbabilityErrFunc, 
                                smoothing=smoothing, seed=seed, verbose=False)
  tmp_df = inverse_res_df.groupby('eps').mean()
  # rename columns for margin function
  model_mean_df.columns = ['fold', 'origin_err', 'marg_err', 'marg_oneC', 
                           'marg_avgC', 'Simul_err', 'Simul_oneC', 'Simul_avgC']
  # these columns contain results for inverse probability non-conformity function
  model_mean_df['inv_err'] = tmp_df['Conf_err']
  model_mean_df['inv_oneC'] = tmp_df['Conf_oneC']
  model_mean_df['inv_avgC'] = tmp_df['Conf_avgC']
  model_mean_df
  return model_mean_df

"""# Setting up

## Setting random seed to get reproducible results & some constants
"""

np.random.seed(seed=1)

num_folds = 10

eps_err = [0.01, 0.03, 0.05, 0.1, 0.2]

"""## Inputs for models

All classifiers for plotting

1 - SVM
"""

# SVM
svm_model_obj=SVC
svm_params={'probability': True}

"""2 - Decision trees"""

dt_model_obj=DecisionTreeClassifier
dt_min_sample_ratio = 0.05 * 0.9 * 0.8  # (5% of the effective learning set)
dt_params={}
MIN_SAMPLES_KEY = 'min_samples_split'

"""3 - KNeighborsClassifier"""

k_kneighbors_model_obj = KNeighborsClassifier
k_kneighbors_params = {"n_neighbors":5}

"""4 - AdaBoostClassifier"""

ada_boost_model_obj = AdaBoostClassifier
ada_boost_params = {}

"""5 - GaussianNB"""

gaussian_nb_model_obj = GaussianNB
gaussian_nb_params = {}

"""6 - input for MLPClassifier"""

mlp_classifier_model_obj = MLPClassifier
mlp_classifier_params = {"alpha":1, "max_iter":1000}

"""7 - RandomForestClassifier"""

random_forest_model_obj  = RandomForestClassifier
random_forest_params = {MIN_SAMPLES_KEY: 0, 'n_estimators': 10}

"""8 - GaussianProcessClassifier"""

gaussian_process_obj = GaussianProcessClassifier
gaussian_process_params = {}

"""9 -  QuadraticDiscriminantAnalysis"""

quadratic_discriminant_model_obj = QuadraticDiscriminantAnalysis
quadratic_discriminant_params = {}

"""## Testing classifiers, which parameters are required to get probability estimates?"""

data = load_iris()

"""1 - SVM"""

model = SVC(probability=True)
model.fit(data['data'], data['target'])
model.predict_proba(data['data'][140:])

"""2 - Decision tree"""

model = DecisionTreeClassifier(min_samples_split=50)
model.fit(data['data'], data['target'])
model.predict_proba(data['data'][60:70])

"""3 - kneighbors_model """

model = KNeighborsClassifier(n_neighbors=10)
model.fit(data['data'], data['target'])
model.predict_proba(data['data'][140:])

"""4- GaussianProcess"""

model = GaussianProcessClassifier()
model.fit(data['data'], data['target'])
model.predict_proba(data['data'][140:])

"""5- RandomForest"""

model =  RandomForestClassifier(max_depth=3,n_estimators=10,max_features=1)
model.fit(data['data'], data['target'])
model.predict_proba(data['data'][140:])

"""6- MLP"""

model =  MLPClassifier(max_iter=1000,alpha=1)
model.fit(data['data'], data['target'])
model.predict_proba(data['data'][140:])

"""7 - AdaBoost"""

model = AdaBoostClassifier()
model.fit(data['data'], data['target'])
model.predict_proba(data['data'][140:])

"""8 - GaussianNB"""

model = GaussianNB()
model.fit(data['data'], data['target'])
model.predict_proba(data['data'][140:])

"""9- QuadraticDiscriminantAnalysis"""

model = QuadraticDiscriminantAnalysis()
model.fit(data['data'], data['target'])
model.predict_proba(data['data'][140:])

"""# Analysis

## Wines both red and white

### Info
"""

wine_R_df = pd.read_csv(path_to_files + 'datasets/winequality-red.csv', sep=';')
print(len(wine_R_df))
wine_R_df.head()

wine_W_df = pd.read_csv(path_to_files + 'datasets/winequality-white.csv', sep=';')
print(len(wine_W_df))
wine_W_df.head()

count_R_df = wine_R_df.groupby('quality').count()[['fixed acidity']]
count_R_df.columns = ['count']
count_W_df = wine_W_df.groupby('quality').count()[['fixed acidity']]
count_W_df.columns = ['count']

labels =sorted(wine_W_df['quality'].unique())
red_c = count_R_df['count'].values.tolist()
red_c.append(0)
#red_c
white_c = count_W_df['count'].values.tolist()

#men_means = [20, 34, 30, 35, 27]
#women_means = [25, 32, 34, 20, 25]

x = np.arange(len(labels))  # the label locations
width = 0.4  # the width of the bars

fig, ax = plt.subplots()
rects1 = ax.bar(x - width/2, red_c, width,   label='Red    - 1599')
rects2 = ax.bar(x + width/2, white_c, width, label='White - 4898')

# Add some text for labels, title and custom x-axis tick labels, etc.
ax.set_ylabel('Number of instances')
ax.set_title('Distribution between classes')
ax.set_xticks(x)
ax.set_xticklabels(labels)
ax.legend()

autolabel(rects1)
autolabel(rects2)

fig.tight_layout()

plt.show()

"""### Run conformal prediction for red wine"""

data = load_wine_red()

"""1- SVM"""

svm_mean_df = analysis_with_2_func(data=data, model_obj=svm_model_obj, 
                     params=svm_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
svm_mean_df

"""2 - Decision trees"""

dt_mean_df = analysis_with_2_func(data=data, model_obj=dt_model_obj, 
                     params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))}, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
dt_mean_df

"""3 - KNeighbors"""

knn_mean_df = analysis_with_2_func(data=data, model_obj=k_kneighbors_model_obj, 
                     params=k_kneighbors_params, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
knn_mean_df

"""4 - AdaBoostClassifier"""

ada_boost_mean_df = analysis_with_2_func(data=data, model_obj=ada_boost_model_obj, 
                     params=ada_boost_params, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
ada_boost_mean_df

"""5 - GaussianNB"""

gussian_nb_mean_df = analysis_with_2_func(data=data,model_obj=gaussian_nb_model_obj, 
                     params=gaussian_nb_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
gussian_nb_mean_df

"""6 - MLPClassifier"""

mlp_classifier_mean_df = analysis_with_2_func(data=data, model_obj=mlp_classifier_model_obj, 
                     params=mlp_classifier_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
mlp_classifier_mean_df

"""7 - RandomForestClassifier"""

random_forest_params[MIN_SAMPLES_KEY] = max(5, int(dt_min_sample_ratio * len(data['data'])))
random_forest_params

random_forest_mean_df = analysis_with_2_func(data=data, model_obj=random_forest_model_obj, 
                     params=random_forest_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
random_forest_mean_df

"""8 - GaussianProcessClassifier"""

gaussian_process_mean_df = pd.DataFrame()

gaussian_process_mean_df = analysis_with_2_func(data=data, model_obj=gaussian_process_obj, 
                     params=gaussian_process_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
gaussian_process_mean_df

"""9 - QuadraticDiscriminantAnalysis"""

quadratic_discriminant_mean_df = analysis_with_2_func(data=data, model_obj=quadratic_discriminant_model_obj, 
                     params=quadratic_discriminant_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
quadratic_discriminant_mean_df

"""### Plot results for RedWine"""

# give names to dataFrames
svm_mean_df.name = 'SVM'
dt_mean_df.name = 'DecisionTree'
knn_mean_df.name = 'KNN'
ada_boost_mean_df.name = 'AdaBoost'
gussian_nb_mean_df.name = 'GaussianNB'
mlp_classifier_mean_df.name = 'Multilayer Perceptron'
random_forest_mean_df.name = 'Random Forest'
gaussian_process_mean_df.name = 'Gaussian process'
quadratic_discriminant_mean_df.name = 'Quadratic discriminant analysis'

all_results = [svm_mean_df, dt_mean_df, knn_mean_df, ada_boost_mean_df, 
               gussian_nb_mean_df, mlp_classifier_mean_df, random_forest_mean_df,
               gaussian_process_mean_df, quadratic_discriminant_mean_df
               ]

"""Error rate"""

k = 4
print('Plotting for {}'.format(all_results[k-1].name))

plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=['origin_err', 'marg_err', 'inv_err', 'Simul_err'], 
          title_str='Error rate', to_extend=[0.3, 0.4, 0.5])

"""Plot: oneC"""

plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=[None, 'marg_oneC', 'inv_oneC', 'Simul_oneC'], 
          title_str='oneC')#, to_extend=[0.3, 0.4, 0.5])

"""avgC"""

plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=[None, 'marg_avgC', 'inv_avgC', 'Simul_avgC'], 
          title_str='avgC')#, to_extend=[0.3, 0.4, 0.5])

"""<font color='red'>Results: Which non-conformity function is better for which metric (oneC should be hight, avgC shold be low)</font>

|   |                        | avgC      | oneC      |
|---|---                     |---        |---        |
| 1 | SVM                    | inv_prob  | margin    |
| 2 | Decision trees         | inv_prob  | margin    |
| 3 | KNN                    | inv_prob  | margin    |
| 4 | AdaBoost               | -----     | ------    |
| 5 | GaussianNB             | inv_prob  | margin    |
| 6 | Multilayer Perceptron  | inv_prob  | margin    |
| 7 | Random Forest          | inv_prob  | margin    |
| 8 | Gaussian process       | inv_prob  | margin    |
| 9 | Quadratic discriminant analysis  | inv_prob  | margin    |

### Run conformal prediction for white wine
"""

data = load_wine_white()

"""##### MarginErrFunc"""

res_df = run_conformal(data, epsilon_arr=eps_err, err_func=MarginErrFunc, smoothing=True,
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
mean_df = res_df.groupby('eps').mean()
mean_df

"""#####  InverseProbabilityErrFunc"""

res_df = run_conformal(data, epsilon_arr=eps_err, err_func=InverseProbabilityErrFunc, smoothing=True,
                             seed=1, verbose=True)
# Get average values of metrics
print('average values for each eps')
res_df.groupby('eps').mean()

tmp_df = res_df.groupby('eps').mean()
mean_df['inv_err'] = tmp_df['Conf_err']
mean_df['inv_oneC'] = tmp_df['Conf_oneC']
mean_df['inv_avgC'] = tmp_df['Conf_avgC']
mean_df

"""<font color='red'>Inv_prob - avgC, margin - oneC</font>

decision tree

MarginErrFunc
"""

res_df = run_conformal(data, model_obj=dt_model_obj, 
                       params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))},
                       epsilon_arr=eps_err, err_func=MarginErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
dt_mean_df = res_df.groupby('eps').mean()
dt_mean_df

"""InverseProbabilityErrFunc"""

res_df = run_conformal(data, model_obj=dt_model_obj, 
                       params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))},
                       epsilon_arr=eps_err, err_func=InverseProbabilityErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
res_df.groupby('eps').mean()

tmp_df = res_df.groupby('eps').mean()
# rename columns for margin function
dt_mean_df.columns = ['fold', 'origin_err', 'marg_err', 'marg_oneC', 'marg_avgC', 
                       'Simul_err', 'Simul_oneC', 'Simul_avgC']
# these columns contain results for inverse probability non-conformity function
dt_mean_df['inv_err'] = tmp_df['Conf_err']
dt_mean_df['inv_oneC'] = tmp_df['Conf_oneC']
dt_mean_df['inv_avgC'] = tmp_df['Conf_avgC']
dt_mean_df

"""Plot results: error rate"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['origin_err'], label='SVM', color=color, lw=3) # thik line for original error
plt.plot(eps_err, df['marg_err'], label='- margin', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_err'], label='- inv prob', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_err'], label='- simulated', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['origin_err'], label='Decision tree', color=color, lw=3) # thik line for original error
plt.plot(eps_err, df['marg_err'], label='_nolegend_', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_err'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_err'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
y_ticks = eps_err[:]
y_ticks.extend([0.3, 0.4, 0.5])
plt.yticks(y_ticks)
plt.grid(True)
plt.legend()
plt.title('Error rate')

"""Plot: oneC"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['marg_oneC'], label='SVM', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_oneC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_oneC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['marg_oneC'], label='Decision tree', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_oneC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_oneC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
plt.grid(True)
plt.legend()
plt.title('oneC')

"""Plot: avgC"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['marg_avgC'], label='SVM', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_avgC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_avgC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['marg_avgC'], label='Decision tree', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_avgC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_avgC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
plt.grid(True)
plt.legend()
plt.title('avgC')

"""## Breast cancer"""

data = load_breast_cancer();
data_df = pd.DataFrame(data['data'])
data_df.columns = data['feature_names']
data_df['target'] = data['target']
print(len(data_df))
data_df.head()

labels = ['malignant', 'benign']
vals = [212, 357]
x = np.arange(len(labels))  # the label locations
width = 0.8  # the width of the bars
fig, ax = plt.subplots()
rects1 = ax.bar(x, vals, width,   label='Breast cancer - 569')
# Add some text for labels, title and custom x-axis tick labels, etc.
ax.set_ylabel('Number of instances')
ax.set_title('Distribution between classes')
ax.set_xticks(x)
ax.set_xticklabels(labels)
ax.legend()
autolabel(rects1)
#autolabel(rects2)
fig.tight_layout()
plt.show()

"""### Run conformal prediction"""

data = load_breast_cancer()

"""#### 1- svm"""

svm_mean_df = analysis_with_2_func(data=data, model_obj=svm_model_obj, 
                     params=svm_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
svm_mean_df

"""##### 2- decision tree"""

dt_mean_df = analysis_with_2_func(data=data, model_obj=dt_model_obj, 
                     params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))}, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
dt_mean_df

"""##### 3-  KNeighbors"""

knn_mean_df = analysis_with_2_func(data=data, model_obj=k_kneighbors_model_obj, 
                     params=k_kneighbors_params, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
knn_mean_df

"""##### 4-  AdaBoostClassifier"""

ada_boost_mean_df = analysis_with_2_func(data=data, model_obj=ada_boost_model_obj, 
                     params=ada_boost_params, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
ada_boost_mean_df

"""##### 5- GaussianNB"""

gussian_nb_mean_df = analysis_with_2_func(data=data,model_obj=gaussian_nb_model_obj, 
                     params=gaussian_nb_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
gussian_nb_mean_df

"""##### 6- MLPClassifier"""

mlp_classifier_mean_df = analysis_with_2_func(data=data, model_obj=mlp_classifier_model_obj, 
                     params=mlp_classifier_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
mlp_classifier_mean_df

"""7 - RandomForestClassifier"""

random_forest_params[MIN_SAMPLES_KEY] = max(5, int(dt_min_sample_ratio * len(data['data'])))
random_forest_params

random_forest_mean_df = analysis_with_2_func(data=data, model_obj=random_forest_model_obj, 
                     params=random_forest_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
random_forest_mean_df

"""##### 8 - GaussianProcessClassifier"""

gaussian_process_mean_df = pd.DataFrame()

gaussian_process_mean_df = analysis_with_2_func(data=data, model_obj=gaussian_process_obj, 
                     params=gaussian_process_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
gaussian_process_mean_df

"""##### 9 - QuadraticDiscriminantAnalysis"""

quadratic_discriminant_mean_df = analysis_with_2_func(data=data, model_obj=quadratic_discriminant_model_obj, 
                     params=quadratic_discriminant_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
quadratic_discriminant_mean_df

"""### Plot results for Breast cancer"""

# give names to dataFrames
svm_mean_df.name = 'SVM'
dt_mean_df.name = 'DecisionTree'
knn_mean_df.name = 'KNN'
ada_boost_mean_df.name = 'AdaBoost'
gussian_nb_mean_df.name = 'GaussianNB'
mlp_classifier_mean_df.name = 'Multilayer Perceptron'
random_forest_mean_df.name = 'Random Forest'
gaussian_process_mean_df.name = 'Gaussian process'
quadratic_discriminant_mean_df.name = 'Quadratic discriminant analysis'

all_results = [svm_mean_df, dt_mean_df, knn_mean_df, ada_boost_mean_df, 
               gussian_nb_mean_df, mlp_classifier_mean_df, random_forest_mean_df,
               gaussian_process_mean_df, quadratic_discriminant_mean_df
               ]

"""##### Error rate"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=['origin_err', 'marg_err', 'inv_err', 'Simul_err'], 
          title_str='Error rate', to_extend=[0.3, 0.4, 0.5])

"""#### Plot: oneC"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=[None, 'marg_oneC', 'inv_oneC', 'Simul_oneC'], 
          title_str='oneC')#, to_extend=[0.3, 0.4, 0.5])

"""### avgC"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=[None, 'marg_avgC', 'inv_avgC', 'Simul_avgC'], 
          title_str='avgC')#, to_extend=[0.3, 0.4, 0.5])

"""|   |                        | avgC      | oneC      |
|---|---                     |---        |---        |
| 1 | SVM                    | inv_prob  | margin    |
| 2 | Decision trees         | inv_prob  | margin    |
| 3 | KNN                    | inv_prob  | margin    |
| 4 | AdaBoost               | inv_prob  | margin    |
| 5 | GaussianNB             | margin    | margin    |
| 6 | Multilayer Perceptron  | inv_prob  | inv_prob  |
| 7 | Random Forest          | inv_prob  | margin    |
| 8 | Gaussian process       | inv_prob  | inv_prob   |
| 9 | Quadratic discriminant analysis  | inv_prob  | margin     |

# Run conformal prediction for bearst cancer

svm

##### MarginErrFunc
"""

res_df = run_conformal(data, epsilon_arr=eps_err, err_func=MarginErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
mean_df = res_df.groupby('eps').mean()
mean_df

"""##### InverseProbabilityErrFunc"""

res_df = run_conformal(data, epsilon_arr=eps_err, err_func=InverseProbabilityErrFunc, smoothing=True, 
                             seed=1, verbose=True)
# Get average values of metrics
print('average values for each eps')
res_df.groupby('eps').mean()

tmp_df = res_df.groupby('eps').mean()
mean_df['inv_err'] = tmp_df['Conf_err']
mean_df['inv_oneC'] = tmp_df['Conf_oneC']
mean_df['inv_avgC'] = tmp_df['Conf_avgC']
mean_df

"""##### decision tree

##### MarginErrFunc
"""

res_df = run_conformal(data, model_obj=dt_model_obj, 
                       params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))},
                       epsilon_arr=eps_err, err_func=MarginErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
dt_mean_df = res_df.groupby('eps').mean()
dt_mean_df

"""InverseProbabilityErrFunc"""

res_df = run_conformal(data, model_obj=dt_model_obj, 
                       params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))},
                       epsilon_arr=eps_err, err_func=InverseProbabilityErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
res_df.groupby('eps').mean()

tmp_df = res_df.groupby('eps').mean()
# rename columns for margin function
dt_mean_df.columns = ['fold', 'origin_err', 'marg_err', 'marg_oneC', 'marg_avgC', 
                       'Simul_err', 'Simul_oneC', 'Simul_avgC']
# these columns contain results for inverse probability non-conformity function
dt_mean_df['inv_err'] = tmp_df['Conf_err']
dt_mean_df['inv_oneC'] = tmp_df['Conf_oneC']
dt_mean_df['inv_avgC'] = tmp_df['Conf_avgC']
dt_mean_df

"""## Plot results: error rate"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['origin_err'], label='SVM', color=color, lw=3) # thik line for original error
plt.plot(eps_err, df['marg_err'], label='- margin', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_err'], label='- inv prob', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_err'], label='- simulated', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['origin_err'], label='Decision tree', color=color, lw=3) # thik line for original error
plt.plot(eps_err, df['marg_err'], label='_nolegend_', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_err'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_err'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
y_ticks = eps_err[:]
#y_ticks.extend([0.3, 0.4, 0.5])
plt.yticks(y_ticks)
plt.grid(True)
plt.legend()
plt.title('Error rate')

"""## Plot: oneC"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['marg_oneC'], label='SVM', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_oneC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_oneC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['marg_oneC'], label='Decision tree', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_oneC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_oneC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
plt.grid(True)
plt.legend()
plt.title('oneC')

"""## Plot: avgC"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['marg_avgC'], label='SVM', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_avgC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_avgC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['marg_avgC'], label='Decision tree', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_avgC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_avgC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
plt.grid(True)
plt.legend()
plt.title('avgC')

"""### Iris"""

data = load_iris();
data_df = pd.DataFrame(data['data'])
data_df.columns = data['feature_names']
data_df['target'] = data['target']
print(len(data_df))
data_df.head()

data_df.groupby('target').count()

labels = data['target_names']
vals = [50, 50, 50]
x = np.arange(len(labels))  # the label locations
width = 0.8  # the width of the bars
fig, ax = plt.subplots()
rects1 = ax.bar(x, vals, width,   label='Iris - 150')
# Add some text for labels, title and custom x-axis tick labels, etc.
ax.set_ylabel('Number of instances')
ax.set_title('Distribution between classes')
ax.set_xticks(x)
ax.set_xticklabels(labels)
ax.legend()
autolabel(rects1)
#autolabel(rects2)
fig.tight_layout()
plt.show()

"""### Run conformal prediction"""

data = load_iris()

"""##### 1-svm"""

svm_mean_df = analysis_with_2_func(data=data, model_obj=svm_model_obj, 
                     params=svm_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
svm_mean_df

"""##### 2- decision tree"""

dt_mean_df = analysis_with_2_func(data=data, model_obj=dt_model_obj, 
                     params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))}, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
dt_mean_df

"""##### 3- KNeighbors"""

knn_mean_df = analysis_with_2_func(data=data, model_obj=k_kneighbors_model_obj, 
                     params=k_kneighbors_params, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
knn_mean_df

"""##### 4 - AdaBoostClassifier"""

ada_boost_mean_df = analysis_with_2_func(data=data, model_obj=ada_boost_model_obj, 
                     params=ada_boost_params, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
ada_boost_mean_df

"""##### 5 - GaussianNB"""

gussian_nb_mean_df = analysis_with_2_func(data=data,model_obj=gaussian_nb_model_obj, 
                     params=gaussian_nb_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
gussian_nb_mean_df

"""##### 6 - MLPClassifier"""

mlp_classifier_mean_df = analysis_with_2_func(data=data, model_obj=mlp_classifier_model_obj, 
                     params=mlp_classifier_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
mlp_classifier_mean_df

"""##### 7 - RandomForestClassifier"""

random_forest_params[MIN_SAMPLES_KEY] = max(5, int(dt_min_sample_ratio * len(data['data'])))
random_forest_params

random_forest_mean_df = analysis_with_2_func(data=data, model_obj=random_forest_model_obj, 
                     params=random_forest_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
random_forest_mean_df

"""##### 8 - GaussianProcessClassifier"""

gaussian_process_mean_df = pd.DataFrame()

gaussian_process_mean_df = analysis_with_2_func(data=data, model_obj=gaussian_process_obj, 
                     params=gaussian_process_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
gaussian_process_mean_df

"""##### 9 - QuadraticDiscriminantAnalysis"""

quadratic_discriminant_mean_df = analysis_with_2_func(data=data, model_obj=quadratic_discriminant_model_obj, 
                     params=quadratic_discriminant_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
quadratic_discriminant_mean_df

"""# Plot results for IRIS"""

# give names to dataFrames
svm_mean_df.name = 'SVM'
dt_mean_df.name = 'DecisionTree'
knn_mean_df.name = 'KNN'
ada_boost_mean_df.name = 'AdaBoost'
gussian_nb_mean_df.name = 'GaussianNB'
mlp_classifier_mean_df.name = 'Multilayer Perceptron'
random_forest_mean_df.name = 'Random Forest'
gaussian_process_mean_df.name = 'Gaussian process'
quadratic_discriminant_mean_df.name = 'Quadratic discriminant analysis'

all_results = [svm_mean_df, dt_mean_df, knn_mean_df, ada_boost_mean_df, 
               gussian_nb_mean_df, mlp_classifier_mean_df, random_forest_mean_df,
               gaussian_process_mean_df, quadratic_discriminant_mean_df
               ]

"""### Error rate"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=['origin_err', 'marg_err', 'inv_err', 'Simul_err'], 
          title_str='Error rate', to_extend=[0.3, 0.4, 0.5])

"""### Plot: oneC"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=[None, 'marg_oneC', 'inv_oneC', 'Simul_oneC'], 
          title_str='oneC')#, to_extend=[0.3, 0.4, 0.5])

"""### avgC"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=[None, 'marg_avgC', 'inv_avgC', 'Simul_avgC'], 
          title_str='avgC')#, to_extend=[0.3, 0.4, 0.5])

"""<font color='red'>Results: Which non-conformity function is better for which metric (oneC should be hight, avgC shold be low)</font>

|   |                        | avgC      | oneC      |
|---|---                     |---        |---        |
| 1 | SVM                    | inv_prob  | margin    |
| 2 | Decision trees         | inv_prob  | margin    |
| 3 | KNN                    | inv_prob  | margin    |
| 4 | AdaBoost               | inv_prob    | inv_prob    |
| 5 | GaussianNB             | inv_prob  | inv_prob    |
| 6 | Multilayer Perceptron  | inv_prob  | inv_prob    |
| 7 | Random Forest          | inv_prob  | margin    |
| 8 | Gaussian process       | inv_prob  | margin    |
| 9 | Quadratic discriminant analysis  | inv_prob  | margin    |

### Run conformal prediction for IRIS

### svm

##### MarginErrFunc
"""

res_df = run_conformal(data, epsilon_arr=eps_err, err_func=MarginErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
mean_df = res_df.groupby('eps').mean()
mean_df

"""#####  InverseProbabilityErrFunc"""

res_df = run_conformal(data, epsilon_arr=eps_err, err_func=InverseProbabilityErrFunc, smoothing=True, 
                             seed=1, verbose=True)
# Get average values of metrics
print('average values for each eps')
res_df.groupby('eps').mean()

tmp_df = res_df.groupby('eps').mean()
mean_df['inv_err'] = tmp_df['Conf_err']
mean_df['inv_oneC'] = tmp_df['Conf_oneC']
mean_df['inv_avgC'] = tmp_df['Conf_avgC']
mean_df

"""### decision tree

##### MarginErrFunc
"""

res_df = run_conformal(data, model_obj=dt_model_obj, 
                       params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))},
                       epsilon_arr=eps_err, err_func=MarginErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
dt_mean_df = res_df.groupby('eps').mean()
dt_mean_df

"""InverseProbabilityErrFunc"""

res_df = run_conformal(data, model_obj=dt_model_obj, 
                       params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))},
                       epsilon_arr=eps_err, err_func=InverseProbabilityErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
res_df.groupby('eps').mean()

tmp_df = res_df.groupby('eps').mean()
# rename columns for margin function
dt_mean_df.columns = ['fold', 'origin_err', 'marg_err', 'marg_oneC', 'marg_avgC', 
                       'Simul_err', 'Simul_oneC', 'Simul_avgC']
# these columns contain results for inverse probability non-conformity function
dt_mean_df['inv_err'] = tmp_df['Conf_err']
dt_mean_df['inv_oneC'] = tmp_df['Conf_oneC']
dt_mean_df['inv_avgC'] = tmp_df['Conf_avgC']
dt_mean_df

"""Plot results: error rate"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['origin_err'], label='SVM', color=color, lw=3) # thik line for original error
plt.plot(eps_err, df['marg_err'], label='- margin', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_err'], label='- inv prob', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_err'], label='- simulated', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['origin_err'], label='Decision tree', color=color, lw=3) # thik line for original error
plt.plot(eps_err, df['marg_err'], label='_nolegend_', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_err'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_err'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
y_ticks = eps_err[:]
y_ticks.extend([0.3, 0.4, 0.5])
plt.yticks(y_ticks)
plt.grid(True)
plt.legend()
plt.title('Error rate')

"""Plot: oneC"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['marg_oneC'], label='SVM', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_oneC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_oneC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['marg_oneC'], label='Decision tree', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_oneC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_oneC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
plt.grid(True)
plt.legend()
plt.title('oneC')

"""avgC"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['marg_avgC'], label='SVM', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_avgC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_avgC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['marg_avgC'], label='Decision tree', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_avgC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_avgC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
plt.grid(True)
plt.legend()
plt.title('avgC')

"""# ecoli dataset"""

def my_load_df(file_name, sep, class_idx=-1, cols_to_exclude=None, header=None):
    df = pd.read_csv(file_name,header=header, sep=sep)
    if cols_to_exclude != None:
        df = df.loc[~df[df.columns[class_idx]].isin(cols_to_exclude)].copy()
        pass
    #print(len(df))
    return df

from sklearn.preprocessing import LabelEncoder
def load_ecoli():
    print('Loading ecoli')
    # load ecoli dataset
    file =path_to_files + 'datasets/ecoli.csv'
    df = my_load_df(path_to_files + 'datasets/ecoli.csv', sep=',', class_idx=-1, 
                      cols_to_exclude=['imL', 'imS', 'omL'], 
                    header=None)
    df.drop(0, axis=1,inplace=True)
    print(df.dtypes)
    target_names = df[df.columns[-1]].unique()
    target_names.sort()
    
    target = df[df.columns[-1]].values
    target_encoder = LabelEncoder()
    target = target_encoder.fit_transform(target) # convert cats to int 
    
    # scale target 
    target -= min(target)
    data = {'target': target, 'data': df[df.columns[:-1]].values,
            'target_names': target_names}
    return data

#ecoli_df = pd.read_csv('../datasets/ecoli.csv',header=None,sep=',')
ecoli_df = my_load_df(path_to_files + 'datasets/ecoli.csv', sep=',', class_idx=-1, 
                      cols_to_exclude=['imL', 'imS', 'omL'], header=None)
print(len(ecoli_df))
ecoli_df.drop(0, axis=1,inplace=True)


ecoli_df.head()

data = load_ecoli()

def plotClasses(df):
    cols = ["col"+str(x) for x in range(1,len(df.columns))]
    cols.append("target")
    df.columns = cols # add column names to the data frame 
    counts = df.groupby(by="target").count()[['col1']]
    labels = counts.index.to_list()
    data_points = counts.values.reshape(len(counts),)
    return plt.bar(labels,data_points)

plotClasses(ecoli_df)  ## call this method on any data frame

"""##### 1- svm"""

svm_mean_df = analysis_with_2_func(data=data, model_obj=svm_model_obj, 
                     params=svm_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
svm_mean_df

"""##### 2- decision tree"""

dt_mean_df = analysis_with_2_func(data=data, model_obj=dt_model_obj, 
                     params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))}, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
dt_mean_df

"""##### 3 - KNeighbors"""

knn_mean_df = analysis_with_2_func(data=data, model_obj=k_kneighbors_model_obj, 
                     params=k_kneighbors_params, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
knn_mean_df

"""##### 4 - AdaBoostClassifier"""

ada_boost_mean_df = analysis_with_2_func(data=data, model_obj=ada_boost_model_obj, 
                     params=ada_boost_params, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
ada_boost_mean_df

"""##### 5 - GaussianNB"""

gussian_nb_mean_df = analysis_with_2_func(data=data,model_obj=gaussian_nb_model_obj, 
                     params=gaussian_nb_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
gussian_nb_mean_df

"""##### 6 - MLPClassifier"""

mlp_classifier_mean_df = analysis_with_2_func(data=data, model_obj=mlp_classifier_model_obj, 
                     params=mlp_classifier_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
mlp_classifier_mean_df

"""##### 7 - RandomForestClassifier"""

random_forest_params[MIN_SAMPLES_KEY] = max(5, int(dt_min_sample_ratio * len(data['data'])))
random_forest_params

random_forest_mean_df = analysis_with_2_func(data=data, model_obj=random_forest_model_obj, 
                     params=random_forest_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
random_forest_mean_df

"""##### 8 - GaussianProcessClassifier"""

gaussian_process_mean_df = pd.DataFrame()

gaussian_process_mean_df = analysis_with_2_func(data=data, model_obj=gaussian_process_obj, 
                     params=gaussian_process_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
gaussian_process_mean_df

"""##### 9 - QuadraticDiscriminantAnalysis"""

quadratic_discriminant_mean_df = analysis_with_2_func(data=data, model_obj=quadratic_discriminant_model_obj, 
                     params=quadratic_discriminant_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
quadratic_discriminant_mean_df

"""### Plot results for ecoli"""

# give names to dataFrames
svm_mean_df.name = 'SVM'
dt_mean_df.name = 'DecisionTree'
knn_mean_df.name = 'KNN'
ada_boost_mean_df.name = 'AdaBoost'
gussian_nb_mean_df.name = 'GaussianNB'
mlp_classifier_mean_df.name = 'Multilayer Perceptron'
random_forest_mean_df.name = 'Random Forest'
gaussian_process_mean_df.name = 'Gaussian process'
quadratic_discriminant_mean_df.name = 'Quadratic discriminant analysis'

all_results = [svm_mean_df, dt_mean_df, knn_mean_df, ada_boost_mean_df, 
               gussian_nb_mean_df, mlp_classifier_mean_df, random_forest_mean_df,
               gaussian_process_mean_df, quadratic_discriminant_mean_df
               ]

"""##### Error rate"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=['origin_err', 'marg_err', 'inv_err', 'Simul_err'], 
          title_str='Error rate', to_extend=[0.3, 0.4, 0.5])

"""##### Plot: oneC"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=[None, 'marg_oneC', 'inv_oneC', 'Simul_oneC'], 
          title_str='oneC')#, to_extend=[0.3, 0.4, 0.5])

"""##### avgC"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=[None, 'marg_avgC', 'inv_avgC', 'Simul_avgC'], 
          title_str='avgC')#, to_extend=[0.3, 0.4, 0.5])

"""<font color='red'>Results: Which non-conformity function is better for which metric (oneC should be hight, avgC shold be low)</font>

|   |                        | avgC      | oneC      |
|---|---                     |---        |---        |
| 1 | SVM                    | inv_prob  | margin    |
| 2 | Decision trees         | inv_prob  | margin    |
| 3 | KNN                    | inv_prob  | margin    |
| 4 | AdaBoost               | inv_prob     | margin    |
| 5 | GaussianNB             | inv_prob  | margin    |
| 6 | Multilayer Perceptron  | inv_prob  | margin    |
| 7 | Random Forest          | inv_prob  | margin    |
| 8 | Gaussian process       | inv_prob  | margin    |
| 9 | Quadratic discriminant analysis  | inv_prob  | margin    |

### svm

##### MarginErrFunc
"""

res_df = run_conformal(data, epsilon_arr=eps_err, err_func=MarginErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
mean_df = res_df.groupby('eps').mean()

"""#####  InverseProbabilityErrFunc"""

res_df = run_conformal(data, epsilon_arr=eps_err, err_func=InverseProbabilityErrFunc, smoothing=True,
                             seed=1, verbose=True)
# Get average values of metrics
print('average values for each eps')
res_df.groupby('eps').mean()

# <font color='red'>Inv_prob - avgC, margin - oneC</font>
tmp_df = res_df.groupby('eps').mean()
mean_df['inv_err'] = tmp_df['Conf_err']
mean_df['inv_oneC'] = tmp_df['Conf_oneC']
mean_df['inv_avgC'] = tmp_df['Conf_avgC']
mean_df

"""# decision tree

MarginErrFunc
"""

res_df = run_conformal(data, model_obj=dt_model_obj, 
                       params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))},
                       epsilon_arr=eps_err, err_func=MarginErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
dt_mean_df = res_df.groupby('eps').mean()
dt_mean_df

"""InverseProbabilityErrFunc"""

res_df = run_conformal(data, model_obj=dt_model_obj, 
                       params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))},
                       epsilon_arr=eps_err, err_func=InverseProbabilityErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
res_df.groupby('eps').mean()

tmp_df = res_df.groupby('eps').mean()
# rename columns for margin function
dt_mean_df.columns = ['fold', 'origin_err', 'marg_err', 'marg_oneC', 'marg_avgC', 
                       'Simul_err', 'Simul_oneC', 'Simul_avgC']
# these columns contain results for inverse probability non-conformity function
dt_mean_df['inv_err'] = tmp_df['Conf_err']
dt_mean_df['inv_oneC'] = tmp_df['Conf_oneC']
dt_mean_df['inv_avgC'] = tmp_df['Conf_avgC']
dt_mean_df

"""Plot results: error rate"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['origin_err'], label='SVM', color=color, lw=3) # thik line for original error
plt.plot(eps_err, df['marg_err'], label='- margin', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_err'], label='- inv prob', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_err'], label='- simulated', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['origin_err'], label='Decision tree', color=color, lw=3) # thik line for original error
plt.plot(eps_err, df['marg_err'], label='_nolegend_', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_err'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_err'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
y_ticks = eps_err[:]
y_ticks.extend([0.3, 0.4, 0.5])
plt.yticks(y_ticks)
plt.grid(True)
plt.legend()
plt.title('Error rate')

"""Plot: oneC"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['marg_oneC'], label='SVM', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_oneC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_oneC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['marg_oneC'], label='Decision tree', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_oneC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_oneC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
plt.grid(True)
plt.legend()
plt.title('oneC')

"""avgC"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['marg_avgC'], label='SVM', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_avgC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_avgC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['marg_avgC'], label='Decision tree', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_avgC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_avgC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
plt.grid(True)
plt.legend()
plt.title('avgC')

"""### zoo dataset"""

cols_to_exclude=[3, 4, 5, 6, 7]

def load_zoo():
    print('Loading zoo')
    # load ecoli dataset
    file = path_to_files +'datasets/zoo.csv'
    df = my_load_df(path_to_files+'datasets/zoo.csv', sep=',', class_idx=-1, 
                      cols_to_exclude=cols_to_exclude, header=None)
    df.drop(0, axis=1,inplace=True)
    #print(df.dtypes)
    target_names = df[df.columns[-1]].unique()
    target_names.sort()
    target = df[df.columns[-1]].values
    #print(target)
    target -= min(target)
    data = {'target': target, 'data': df[df.columns[:-1]].values,
            'target_names': target_names}
    return data

#zoo_df = pd.read_csv('../datasets/zoo.csv',header=None,sep=',')
zoo_df = my_load_df(path_to_files+ 'datasets/zoo.csv', sep=',', class_idx=-1, 
                      cols_to_exclude=cols_to_exclude, header=None)
print(len(zoo_df))
zoo_df.drop(0, axis=1,inplace=True)
zoo_df.head()

data = load_zoo()

def plotClasses(df):
    cols = ["col"+str(x) for x in range(1,len(df.columns))]
    cols.append("target")
    df.columns = cols # add column names to the data frame 
    counts = df.groupby(by="target").count()[['col1']]
    labels = counts.index.to_list()
    data_points = counts.values.reshape(len(counts),)
    return plt.bar(labels,data_points)

plotClasses(zoo_df)

"""### Run conformal prediction for zoo

#### 1-svm
"""

svm_mean_df = analysis_with_2_func(data=data, model_obj=svm_model_obj, 
                     params=svm_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
svm_mean_df

"""### 2- decision tree"""

dt_mean_df = analysis_with_2_func(data=data, model_obj=dt_model_obj, 
                     params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))}, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
dt_mean_df

"""##### 3 - KNeighbors"""

knn_mean_df = analysis_with_2_func(data=data, model_obj=k_kneighbors_model_obj, 
                     params=k_kneighbors_params, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
knn_mean_df

"""##### 4 - AdaBoostClassifier"""

ada_boost_mean_df = analysis_with_2_func(data=data, model_obj=ada_boost_model_obj, 
                     params=ada_boost_params, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
ada_boost_mean_df

"""#### 5 - GaussianNB"""

gussian_nb_mean_df = analysis_with_2_func(data=data,model_obj=gaussian_nb_model_obj, 
                     params=gaussian_nb_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
gussian_nb_mean_df

"""##### 6 - MLPClassifier"""

mlp_classifier_mean_df = analysis_with_2_func(data=data, model_obj=mlp_classifier_model_obj, 
                     params=mlp_classifier_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
mlp_classifier_mean_df

"""##### 7 - RandomForestClassifier"""

random_forest_params[MIN_SAMPLES_KEY] = max(5, int(dt_min_sample_ratio * len(data['data'])))
random_forest_params

random_forest_mean_df = analysis_with_2_func(data=data, model_obj=random_forest_model_obj, 
                     params=random_forest_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
random_forest_mean_df

"""##### 8 - GaussianProcessClassifier"""

gaussian_process_mean_df = pd.DataFrame()

gaussian_process_mean_df = analysis_with_2_func(data=data, model_obj=gaussian_process_obj, 
                     params=gaussian_process_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
gaussian_process_mean_df

"""##### 9 - QuadraticDiscriminantAnalysis"""

quadratic_discriminant_mean_df = analysis_with_2_func(data=data, model_obj=quadratic_discriminant_model_obj, 
                     params=quadratic_discriminant_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
quadratic_discriminant_mean_df

"""##### Plot results for zoo"""

# give names to dataFrames
svm_mean_df.name = 'SVM'
dt_mean_df.name = 'DecisionTree'
knn_mean_df.name = 'KNN'
ada_boost_mean_df.name = 'AdaBoost'
gussian_nb_mean_df.name = 'GaussianNB'
mlp_classifier_mean_df.name = 'Multilayer Perceptron'
random_forest_mean_df.name = 'Random Forest'
gaussian_process_mean_df.name = 'Gaussian process'
quadratic_discriminant_mean_df.name = 'Quadratic discriminant analysis'

all_results = [svm_mean_df, dt_mean_df, knn_mean_df, ada_boost_mean_df, 
               gussian_nb_mean_df, mlp_classifier_mean_df, random_forest_mean_df,
               gaussian_process_mean_df, quadratic_discriminant_mean_df
               ]

"""##### Error rate"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=['origin_err', 'marg_err', 'inv_err', 'Simul_err'], 
          title_str='Error rate', to_extend=[0.3, 0.4, 0.5])

"""##### oneC"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=[None, 'marg_oneC', 'inv_oneC', 'Simul_oneC'], 
          title_str='oneC')#, to_extend=[0.3, 0.4, 0.5])

"""##### angC"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=[None, 'marg_avgC', 'inv_avgC', 'Simul_avgC'], 
          title_str='avgC')#, to_extend=[0.3, 0.4, 0.5])

"""<font color='red'>Results: Which non-conformity function is better for which metric (oneC should be hight, avgC shold be low)</font>

|   |                        | avgC      | oneC      |
|---|---                     |---        |---        |
| 1 | SVM                    | inv_prob  | margin    |
| 2 | Decision trees         | inv_prob  | margin    |
| 3 | KNN                    | inv_prob  | margin    |
| 4 | AdaBoost               | inv_prob    | margin     |
| 5 | GaussianNB             | inv_prob  | margin    |
| 6 | Multilayer Perceptron  | inv_prob  | margin    |
| 7 | Random Forest          | inv_prob  | margin    |
| 8 | Gaussian process       | inv_prob  | margin    |
| 9 | Quadratic discriminant analysis  | inv_prob  | margin    |

### Run conformal prediction for zoo

#### svm

##### MarginErrFunc
"""

res_df = run_conformal(data, epsilon_arr=eps_err, err_func=MarginErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
mean_df = res_df.groupby('eps').mean()
mean_df

"""#####  InverseProbabilityErrFunc"""

res_df = run_conformal(data, epsilon_arr=eps_err, err_func=InverseProbabilityErrFunc, smoothing=True,
                             seed=1, verbose=True)
# Get average values of metrics
print('average values for each eps')
res_df.groupby('eps').mean()

# <font color='red'>Inv_prob - avgC, margin - oneC</font>
tmp_df = res_df.groupby('eps').mean()
mean_df['inv_err'] = tmp_df['Conf_err']
mean_df['inv_oneC'] = tmp_df['Conf_oneC']
mean_df['inv_avgC'] = tmp_df['Conf_avgC']
mean_df

"""#### decision tree

##### MarginErrFunc
"""

res_df = run_conformal(data, model_obj=dt_model_obj, 
                       params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))},
                       epsilon_arr=eps_err, err_func=MarginErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
dt_mean_df = res_df.groupby('eps').mean()
dt_mean_df

"""InverseProbabilityErrFunc"""

res_df = run_conformal(data, model_obj=dt_model_obj, 
                       params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))},
                       epsilon_arr=eps_err, err_func=InverseProbabilityErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
res_df.groupby('eps').mean()

tmp_df = res_df.groupby('eps').mean()
# rename columns for margin function
dt_mean_df.columns = ['fold', 'origin_err', 'marg_err', 'marg_oneC', 'marg_avgC', 
                       'Simul_err', 'Simul_oneC', 'Simul_avgC']
# these columns contain results for inverse probability non-conformity function
dt_mean_df['inv_err'] = tmp_df['Conf_err']
dt_mean_df['inv_oneC'] = tmp_df['Conf_oneC']
dt_mean_df['inv_avgC'] = tmp_df['Conf_avgC']
dt_mean_df

"""Plot results: error rate"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['origin_err'], label='SVM', color=color, lw=3) # thik line for original error
plt.plot(eps_err, df['marg_err'], label='- margin', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_err'], label='- inv prob', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_err'], label='- simulated', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['origin_err'], label='Decision tree', color=color, lw=3) # thik line for original error
plt.plot(eps_err, df['marg_err'], label='_nolegend_', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_err'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_err'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
y_ticks = eps_err[:]
y_ticks.extend([0.3, 0.4, 0.5])
plt.yticks(y_ticks)
plt.grid(True)
plt.legend()
plt.title('Error rate')

"""Plot: oneC"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['marg_oneC'], label='SVM', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_oneC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_oneC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['marg_oneC'], label='Decision tree', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_oneC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_oneC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
plt.grid(True)
plt.legend()
plt.title('oneC')

"""avgC"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['marg_avgC'], label='SVM', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_avgC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_avgC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['marg_avgC'], label='Decision tree', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_avgC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_avgC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
plt.grid(True)
plt.legend()
plt.title('avgC')

"""## yeast dataset"""

# replace 4 spaces with two spaces in the file 
import fileinput

with fileinput.FileInput(path_to_files + 'datasets/yeast_2.csv', inplace=True, backup='.bak') as file:
    for line in file:
        print(line.replace("    ","  "),end='')

cols_to_exclude = ['ERL', 'POX', 'VAC']

def load_yeast_df(class_idx=-1, cols_to_exclude=None):
    df = pd.read_csv(path_to_files +'datasets/yeast_2.csv',header=None, delim_whitespace=True )
    if cols_to_exclude != None:
        df = df.loc[~df[df.columns[class_idx]].isin(cols_to_exclude)].copy()
        pass
    #print(len(df))
    return df

from sklearn.preprocessing import LabelEncoder
def load_yeast():
    print('Loading yeast')
    # load yeast dataset
    file = path_to_files +'datasets/yeast_2.csv'
    df = load_yeast_df(class_idx=-1,cols_to_exclude=cols_to_exclude)
    df.drop(0, axis=1,inplace=True)
    print(df.dtypes)
    target_names = df[df.columns[-1]].unique()
    target_names.sort()
    
    target = df[df.columns[-1]].values
    target_encoder = LabelEncoder()
    target = target_encoder.fit_transform(target) # convert cats to int 
    
    # scale target 
    target -= min(target)
    data = {'target': target, 'data': df[df.columns[:-1]].values,
            'target_names': target_names}
    return data

#yeast_df = pd.read_csv('../datasets/yeast_2.csv',header=None,sep=' ')
yeast_df = load_yeast_df(class_idx=-1,cols_to_exclude=cols_to_exclude)
print(len(yeast_df))
# yeast_df.drop(0, axis=1,inplace=True)
yeast_df.head()

yeast_df.groupby(yeast_df.columns[-1]).count()

data = load_yeast()

def plotClasses(df):
    cols = ["col"+str(x) for x in range(1,len(df.columns))]
    cols.append("target")
    df.columns = cols # add column names to the data frame 
    counts = df.groupby(by="target").count()[['col1']]
    labels = counts.index.to_list()
    data_points = counts.values.reshape(len(counts),)
    return plt.bar(labels,data_points)

plotClasses(yeast_df)

"""### Run conformal prediction for yaset

1.   List item
2.   List item

##### 1- svm
"""

svm_mean_df = analysis_with_2_func(data=data, model_obj=svm_model_obj, 
                     params=svm_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
svm_mean_df

"""##### 2- decision tree"""

dt_mean_df = analysis_with_2_func(data=data, model_obj=dt_model_obj, 
                     params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))}, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
dt_mean_df

"""##### 3 - KNeighbors"""

knn_mean_df = analysis_with_2_func(data=data, model_obj=k_kneighbors_model_obj, 
                     params=k_kneighbors_params, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
knn_mean_df

"""##### 4 - AdaBoostClassifier"""

ada_boost_mean_df = analysis_with_2_func(data=data, model_obj=ada_boost_model_obj, 
                     params=ada_boost_params, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
ada_boost_mean_df

"""##### 5 - GaussianNB"""

gussian_nb_mean_df = analysis_with_2_func(data=data,model_obj=gaussian_nb_model_obj, 
                     params=gaussian_nb_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
gussian_nb_mean_df

"""##### 6 - MLPClassifier"""

mlp_classifier_mean_df = analysis_with_2_func(data=data, model_obj=mlp_classifier_model_obj, 
                     params=mlp_classifier_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
mlp_classifier_mean_df

"""##### 7 - RandomForestClassifier"""

random_forest_params[MIN_SAMPLES_KEY] = max(5, int(dt_min_sample_ratio * len(data['data'])))
random_forest_params

random_forest_mean_df = analysis_with_2_func(data=data, model_obj=random_forest_model_obj, 
                     params=random_forest_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
random_forest_mean_df

"""#### 8 - GaussianProcessClassifier"""

gaussian_process_mean_df = pd.DataFrame()

gaussian_process_mean_df = analysis_with_2_func(data=data, model_obj=gaussian_process_obj, 
                     params=gaussian_process_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
gaussian_process_mean_df

"""##### 9 - QuadraticDiscriminantAnalysis"""

quadratic_discriminant_mean_df = analysis_with_2_func(data=data, model_obj=quadratic_discriminant_model_obj, 
                     params=quadratic_discriminant_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
quadratic_discriminant_mean_df

"""### Plot results for yaset"""

# give names to dataFrames
svm_mean_df.name = 'SVM'
dt_mean_df.name = 'DecisionTree'
knn_mean_df.name = 'KNN'
ada_boost_mean_df.name = 'AdaBoost'
gussian_nb_mean_df.name = 'GaussianNB'
mlp_classifier_mean_df.name = 'Multilayer Perceptron'
random_forest_mean_df.name = 'Random Forest'
gaussian_process_mean_df.name = 'Gaussian process'
quadratic_discriminant_mean_df.name = 'Quadratic discriminant analysis'

all_results = [svm_mean_df, dt_mean_df, knn_mean_df, ada_boost_mean_df, 
               gussian_nb_mean_df, mlp_classifier_mean_df, random_forest_mean_df,
               gaussian_process_mean_df, quadratic_discriminant_mean_df
               ]

"""##### Error rate"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=['origin_err', 'marg_err', 'inv_err', 'Simul_err'], 
          title_str='Error rate', to_extend=[0.3, 0.4, 0.5])

"""### oneC"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=[None, 'marg_oneC', 'inv_oneC', 'Simul_oneC'], 
          title_str='oneC')#, to_extend=[0.3, 0.4, 0.5])

"""### avgC"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=[None, 'marg_avgC', 'inv_avgC', 'Simul_avgC'], 
          title_str='avgC')#, to_extend=[0.3, 0.4, 0.5])

"""<font color='red'>Results: Which non-conformity function is better for which metric (oneC should be hight, avgC shold be low)</font>

|   |                        | avgC      | oneC      |
|---|---                     |---        |---        |
| 1 | SVM                    | inv_prob  | margin    |
| 2 | Decision trees         | inv_prob  | margin    |
| 3 | KNN                    | inv_prob  | margin    |
| 4 | AdaBoost               | inv_prob     | margin    |
| 5 | GaussianNB             | inv_prob  | margin    |
| 6 | Multilayer Perceptron  | inv_prob  | margin    |
| 7 | Random Forest          | inv_prob  | margin    |
| 8 | Gaussian process       | inv_prob  | margin    |
| 9 | Quadratic discriminant analysis  | inv_prob  | margin    |

### svm

### MarginErrFunc
"""

res_df = run_conformal(data, epsilon_arr=eps_err, err_func=MarginErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
mean_df = res_df.groupby('eps').mean()
mean_df

"""#### InverseProbabilityErrFunc"""

res_df = run_conformal(data, epsilon_arr=eps_err, err_func=InverseProbabilityErrFunc, smoothing=True,
                             seed=1, verbose=True)
# Get average values of metrics
print('average values for each eps')
res_df.groupby('eps').mean()

# <font color='red'>Inv_prob - avgC, margin - oneC</font>
tmp_df = res_df.groupby('eps').mean()
mean_df['inv_err'] = tmp_df['Conf_err']
mean_df['inv_oneC'] = tmp_df['Conf_oneC']
mean_df['inv_avgC'] = tmp_df['Conf_avgC']
mean_df

"""### decision tree

### MarginErrFunc
"""

res_df = run_conformal(data, model_obj=dt_model_obj, 
                       params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))},
                       epsilon_arr=eps_err, err_func=MarginErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
dt_mean_df = res_df.groupby('eps').mean()
dt_mean_df

"""InverseProbabilityErrFunc"""

res_df = run_conformal(data, model_obj=dt_model_obj, 
                       params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))},
                       epsilon_arr=eps_err, err_func=InverseProbabilityErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
res_df.groupby('eps').mean()

tmp_df = res_df.groupby('eps').mean()
# rename columns for margin function
dt_mean_df.columns = ['fold', 'origin_err', 'marg_err', 'marg_oneC', 'marg_avgC', 
                       'Simul_err', 'Simul_oneC', 'Simul_avgC']
# these columns contain results for inverse probability non-conformity function
dt_mean_df['inv_err'] = tmp_df['Conf_err']
dt_mean_df['inv_oneC'] = tmp_df['Conf_oneC']
dt_mean_df['inv_avgC'] = tmp_df['Conf_avgC']
dt_mean_df

"""Plot results: error rate"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['origin_err'], label='SVM', color=color, lw=3) # thik line for original error
plt.plot(eps_err, df['marg_err'], label='- margin', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_err'], label='- inv prob', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_err'], label='- simulated', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['origin_err'], label='Decision tree', color=color, lw=3) # thik line for original error
plt.plot(eps_err, df['marg_err'], label='_nolegend_', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_err'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_err'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
y_ticks = eps_err[:]
y_ticks.extend([0.3, 0.4, 0.5])
plt.yticks(y_ticks)
plt.grid(True)
plt.legend()
plt.title('Error rate')

"""Plot: oneC"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['marg_oneC'], label='SVM', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_oneC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_oneC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['marg_oneC'], label='Decision tree', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_oneC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_oneC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
plt.grid(True)
plt.legend()
plt.title('oneC')

"""avgC"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['marg_avgC'], label='SVM', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_avgC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_avgC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['marg_avgC'], label='Decision tree', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_avgC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_avgC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
plt.grid(True)
plt.legend()
plt.title('avgC')

"""## balance data"""

from sklearn.preprocessing import LabelEncoder
def load_balance():
    print('Loading balance')
    # load balance dataset
    file = path_to_files +'datasets/balance.csv'
    df = pd.read_csv(file,header=None, sep=',')
    df.drop(0, axis=1,inplace=True)
    print(df.dtypes)
    target_names = df[df.columns[-1]].unique()
    target_names.sort()
    
    target = df[df.columns[-1]].values 
    
    # scale target 
    target -= min(target)
    data = {'target': target, 'data': df[df.columns[:-1]].values,
            'target_names': target_names}
    return data

balance_df = pd.read_csv(path_to_files +'datasets/balance.csv',header=None,sep=',')
print(len(balance_df))
# balance_df.drop(0, axis=1,inplace=True)
balance_df.head()

data = load_balance()

def plotClasses(df):
    cols = ["col"+str(x) for x in range(1,len(df.columns))]
    cols.append("target")
    df.columns = cols # add column names to the data frame 
    counts = df.groupby(by="target").count()[['col1']]
    labels = counts.index.to_list()
    data_points = counts.values.reshape(len(counts),)
    return plt.bar(labels,data_points)

plotClasses(balance_df)

"""### Run conformal prediction

##### 1-svm
"""

svm_mean_df = analysis_with_2_func(data=data, model_obj=svm_model_obj, 
                     params=svm_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
svm_mean_df

"""##### 2- decision tree"""

dt_mean_df = analysis_with_2_func(data=data, model_obj=dt_model_obj, 
                     params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))}, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
dt_mean_df

"""###### 3 - KNeighbors"""

knn_mean_df = analysis_with_2_func(data=data, model_obj=k_kneighbors_model_obj, 
                     params=k_kneighbors_params, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
knn_mean_df

"""##### 4 - AdaBoostClassifier"""

ada_boost_mean_df = analysis_with_2_func(data=data, model_obj=ada_boost_model_obj, 
                     params=ada_boost_params, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
ada_boost_mean_df

"""##### 5 - GaussianNB"""

gussian_nb_mean_df = analysis_with_2_func(data=data,model_obj=gaussian_nb_model_obj, 
                     params=gaussian_nb_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
gussian_nb_mean_df

"""##### 6 - MLPClassifier"""

mlp_classifier_mean_df = analysis_with_2_func(data=data, model_obj=mlp_classifier_model_obj, 
                     params=mlp_classifier_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
mlp_classifier_mean_df

"""###### 7 - RandomForestClassifier"""

random_forest_params[MIN_SAMPLES_KEY] = max(5, int(dt_min_sample_ratio * len(data['data'])))
random_forest_params

random_forest_mean_df = analysis_with_2_func(data=data, model_obj=random_forest_model_obj, 
                     params=random_forest_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
random_forest_mean_df

"""##### 8 - GaussianProcessClassifier"""

gaussian_process_mean_df = pd.DataFrame()

gaussian_process_mean_df = analysis_with_2_func(data=data, model_obj=gaussian_process_obj, 
                     params=gaussian_process_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
gaussian_process_mean_df

"""###### 9 - QuadraticDiscriminantAnalysis"""

quadratic_discriminant_mean_df = analysis_with_2_func(data=data, model_obj=quadratic_discriminant_model_obj, 
                     params=quadratic_discriminant_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
quadratic_discriminant_mean_df

"""### Plot results"""

# give names to dataFrames
svm_mean_df.name = 'SVM'
dt_mean_df.name = 'DecisionTree'
knn_mean_df.name = 'KNN'
ada_boost_mean_df.name = 'AdaBoost'
gussian_nb_mean_df.name = 'GaussianNB'
mlp_classifier_mean_df.name = 'Multilayer Perceptron'
random_forest_mean_df.name = 'Random Forest'
gaussian_process_mean_df.name = 'Gaussian process'
quadratic_discriminant_mean_df.name = 'Quadratic discriminant analysis'

all_results = [svm_mean_df, dt_mean_df, knn_mean_df, ada_boost_mean_df, 
               gussian_nb_mean_df, mlp_classifier_mean_df, random_forest_mean_df,
               gaussian_process_mean_df, quadratic_discriminant_mean_df
               ]

"""##### Error rate"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=['origin_err', 'marg_err', 'inv_err', 'Simul_err'], 
          title_str='Error rate', to_extend=[0.3, 0.4, 0.5])

"""#### oneC"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=[None, 'marg_oneC', 'inv_oneC', 'Simul_oneC'], 
          title_str='oneC')#, to_extend=[0.3, 0.4, 0.5])

"""#### avgC"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=[None, 'marg_avgC', 'inv_avgC', 'Simul_avgC'], 
          title_str='avgC')#, to_extend=[0.3, 0.4, 0.5])

"""<font color='red'>Results: Which non-conformity function is better for which metric (oneC should be hight, avgC shold be low)</font>

|   |                        | avgC      | oneC      |
|---|---                     |---        |---        |
| 1 | SVM                    | inv_prob  | margin    |
| 2 | Decision trees         | inv_prob  | margin    |
| 3 | KNN                    | inv_prob  | margin    |
| 4 | AdaBoost               | -----     | ------    |
| 5 | GaussianNB             | inv_prob  | margin    |
| 6 | Multilayer Perceptron  | inv_prob  | margin    |
| 7 | Random Forest          | inv_prob  | margin    |
| 8 | Gaussian process       | inv_prob  | margin    |
| 9 | Quadratic discriminant analysis  | inv_prob  | margin    |

### svm

MarginErrFunc
"""

res_df = run_conformal(data, epsilon_arr=eps_err, err_func=MarginErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
mean_df = res_df.groupby('eps').mean()
mean_df

"""#####  InverseProbabilityErrFunc"""

res_df = run_conformal(data, epsilon_arr=eps_err, err_func=InverseProbabilityErrFunc, smoothing=True,
                             seed=1, verbose=True)
# Get average values of metrics
print('average values for each eps')
res_df.groupby('eps').mean()

# <font color='red'>Inv_prob - avgC, margin - oneC</font>
tmp_df = res_df.groupby('eps').mean()
mean_df['inv_err'] = tmp_df['Conf_err']
mean_df['inv_oneC'] = tmp_df['Conf_oneC']
mean_df['inv_avgC'] = tmp_df['Conf_avgC']
mean_df

"""### decision tree

MarginErrFunc
"""

res_df = run_conformal(data, model_obj=dt_model_obj, 
                       params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))},
                       epsilon_arr=eps_err, err_func=MarginErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
dt_mean_df = res_df.groupby('eps').mean()
dt_mean_df

"""#####  InverseProbabilityErrFunc"""

res_df = run_conformal(data, model_obj=dt_model_obj, 
                       params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))},
                       epsilon_arr=eps_err, err_func=InverseProbabilityErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
res_df.groupby('eps').mean()

tmp_df = res_df.groupby('eps').mean()
# rename columns for margin function
dt_mean_df.columns = ['fold', 'origin_err', 'marg_err', 'marg_oneC', 'marg_avgC', 
                       'Simul_err', 'Simul_oneC', 'Simul_avgC']
# these columns contain results for inverse probability non-conformity function
dt_mean_df['inv_err'] = tmp_df['Conf_err']
dt_mean_df['inv_oneC'] = tmp_df['Conf_oneC']
dt_mean_df['inv_avgC'] = tmp_df['Conf_avgC']
dt_mean_df

"""Plot results: error rate"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['origin_err'], label='SVM', color=color, lw=3) # thik line for original error
plt.plot(eps_err, df['marg_err'], label='- margin', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_err'], label='- inv prob', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_err'], label='- simulated', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['origin_err'], label='Decision tree', color=color, lw=3) # thik line for original error
plt.plot(eps_err, df['marg_err'], label='_nolegend_', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_err'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_err'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
y_ticks = eps_err[:]
y_ticks.extend([0.3, 0.4, 0.5])
plt.yticks(y_ticks)
plt.grid(True)
plt.legend()
plt.title('Error rate')

"""Plot: oneC"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['marg_oneC'], label='SVM', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_oneC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_oneC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['marg_oneC'], label='Decision tree', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_oneC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_oneC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
plt.grid(True)
plt.legend()
plt.title('oneC')

"""### avgC"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['marg_avgC'], label='SVM', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_avgC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_avgC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['marg_avgC'], label='Decision tree', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_avgC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_avgC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
plt.grid(True)
plt.legend()
plt.title('avgC')

"""## glass data"""

cols_to_exclude = [3, 5, 6, 7]

# from sklearn.preprocessing import LabelEncoder
def load_glass():
    print('Loading glass')
    # load glass dataset
    file = path_to_files +'datasets/glass.csv'
    #df = pd.read_csv(file,header=None, sep=',')
    df = my_load_df(path_to_files +'datasets/glass.csv', sep=',', class_idx=-1, 
                      cols_to_exclude=cols_to_exclude, header=None)
    df.drop(0, axis=1,inplace=True)
    print(df.dtypes)
    target_names = df[df.columns[-1]].unique()
    target_names.sort()
    
    target = df[df.columns[-1]].values
    
    
    # scale target 
    target -= min(target)
    data = {'target': target, 'data': df[df.columns[:-1]].values,
            'target_names': target_names}
    return data
#load_glass()

#glass_df = pd.read_csv('../datasets/glass.csv',header=None,sep=',')
glass_df = my_load_df(path_to_files + 'datasets/glass.csv', sep=',', class_idx=-1, 
                      cols_to_exclude=cols_to_exclude, header=None)
print(len(glass_df))
glass_df.drop(0, axis=1,inplace=True)
glass_df.head()

data = load_glass()

def plotClasses(df):
    cols = ["col"+str(x) for x in range(1,len(df.columns))]
    cols.append("target")
    df.columns = cols # add column names to the data frame 
    counts = df.groupby(by="target").count()[['col1']]
    labels = counts.index.to_list()
    data_points = counts.values.reshape(len(counts),)
    return plt.bar(labels,data_points)

plotClasses(glass_df)

"""### Run conformal prediction

##### 1-svm
"""

svm_mean_df = analysis_with_2_func(data=data, model_obj=svm_model_obj, 
                     params=svm_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
svm_mean_df

"""##### 2- decision tree"""

dt_mean_df = analysis_with_2_func(data=data, model_obj=dt_model_obj, 
                     params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))}, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
dt_mean_df

"""##### 3 - KNeighbors"""

knn_mean_df = analysis_with_2_func(data=data, model_obj=k_kneighbors_model_obj, 
                     params=k_kneighbors_params, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
knn_mean_df

"""###### 4 - AdaBoostClassifier"""

ada_boost_mean_df = analysis_with_2_func(data=data, model_obj=ada_boost_model_obj, 
                     params=ada_boost_params, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
ada_boost_mean_df

"""##### 5 - GaussianNB"""

gussian_nb_mean_df = analysis_with_2_func(data=data,model_obj=gaussian_nb_model_obj, 
                     params=gaussian_nb_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
gussian_nb_mean_df

"""##### 6 - MLPClassifier"""

mlp_classifier_mean_df = analysis_with_2_func(data=data, model_obj=mlp_classifier_model_obj, 
                     params=mlp_classifier_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
mlp_classifier_mean_df

"""##### 7 - RandomForestClassifier"""

random_forest_params[MIN_SAMPLES_KEY] = max(5, int(dt_min_sample_ratio * len(data['data'])))
random_forest_params

random_forest_mean_df = analysis_with_2_func(data=data, model_obj=random_forest_model_obj, 
                     params=random_forest_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
random_forest_mean_df

"""##### 8 - GaussianProcessClassifier"""

gaussian_process_mean_df = pd.DataFrame()

gaussian_process_mean_df = analysis_with_2_func(data=data, model_obj=gaussian_process_obj, 
                     params=gaussian_process_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
gaussian_process_mean_df

"""##### 9 - QuadraticDiscriminantAnalysis"""

quadratic_discriminant_mean_df = analysis_with_2_func(data=data, model_obj=quadratic_discriminant_model_obj, 
                     params=quadratic_discriminant_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
quadratic_discriminant_mean_df

"""### Plot results"""

# give names to dataFrames
svm_mean_df.name = 'SVM'
dt_mean_df.name = 'DecisionTree'
knn_mean_df.name = 'KNN'
ada_boost_mean_df.name = 'AdaBoost'
gussian_nb_mean_df.name = 'GaussianNB'
mlp_classifier_mean_df.name = 'Multilayer Perceptron'
random_forest_mean_df.name = 'Random Forest'
gaussian_process_mean_df.name = 'Gaussian process'
quadratic_discriminant_mean_df.name = 'Quadratic discriminant analysis'

all_results = [svm_mean_df, dt_mean_df, knn_mean_df, ada_boost_mean_df, 
               gussian_nb_mean_df, mlp_classifier_mean_df, random_forest_mean_df,
               gaussian_process_mean_df, quadratic_discriminant_mean_df
               ]

"""####  Error rate"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=['origin_err', 'marg_err', 'inv_err', 'Simul_err'], 
          title_str='Error rate', to_extend=[0.3, 0.4, 0.5])

"""#### oneC"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=[None, 'marg_oneC', 'inv_oneC', 'Simul_oneC'], 
          title_str='oneC')#, to_extend=[0.3, 0.4, 0.5])

"""#### avgC"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=[None, 'marg_avgC', 'inv_avgC', 'Simul_avgC'], 
          title_str='avgC')#, to_extend=[0.3, 0.4, 0.5])

"""<font color='red'>Results: Which non-conformity function is better for which metric (oneC should be hight, avgC shold be low)</font>

|   |                        | avgC      | oneC      |
|---|---                     |---        |---        |
| 1 | SVM                    | inv_prob  | margin    |
| 2 | Decision trees         | inv_prob  | margin    |
| 3 | KNN                    | inv_prob  | margin    |
| 4 | AdaBoost               | -----     | ------    |
| 5 | GaussianNB             | inv_prob  | margin    |
| 6 | Multilayer Perceptron  | inv_prob  | margin    |
| 7 | Random Forest          | inv_prob  | margin    |
| 8 | Gaussian process       | inv_prob  | margin    |
| 9 | Quadratic discriminant analysis  | inv_prob  | margin    |

### svm

##### MarginErrFunc
"""

res_df = run_conformal(data, epsilon_arr=eps_err, err_func=MarginErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
mean_df = res_df.groupby('eps').mean()
mean_df

"""#####  InverseProbabilityErrFunc"""

res_df = run_conformal(data, epsilon_arr=eps_err, err_func=InverseProbabilityErrFunc, smoothing=True,
                             seed=1, verbose=True)
# Get average values of metrics
print('average values for each eps')
res_df.groupby('eps').mean()

# <font color='red'>Inv_prob - avgC, margin - oneC</font>
tmp_df = res_df.groupby('eps').mean()
mean_df['inv_err'] = tmp_df['Conf_err']
mean_df['inv_oneC'] = tmp_df['Conf_oneC']
mean_df['inv_avgC'] = tmp_df['Conf_avgC']
mean_df

"""### decision tree

##### MarginErrFunc
"""

res_df = run_conformal(data, model_obj=dt_model_obj, 
                       params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))},
                       epsilon_arr=eps_err, err_func=MarginErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
dt_mean_df = res_df.groupby('eps').mean()
dt_mean_df

"""#####  InverseProbabilityErrFunc"""

res_df = run_conformal(data, model_obj=dt_model_obj, 
                       params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))},
                       epsilon_arr=eps_err, err_func=InverseProbabilityErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
res_df.groupby('eps').mean()

tmp_df = res_df.groupby('eps').mean()
# rename columns for margin function
dt_mean_df.columns = ['fold', 'origin_err', 'marg_err', 'marg_oneC', 'marg_avgC', 
                       'Simul_err', 'Simul_oneC', 'Simul_avgC']
# these columns contain results for inverse probability non-conformity function
dt_mean_df['inv_err'] = tmp_df['Conf_err']
dt_mean_df['inv_oneC'] = tmp_df['Conf_oneC']
dt_mean_df['inv_avgC'] = tmp_df['Conf_avgC']
dt_mean_df

"""Plot results: error rate"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['origin_err'], label='SVM', color=color, lw=3) # thik line for original error
plt.plot(eps_err, df['marg_err'], label='- margin', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_err'], label='- inv prob', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_err'], label='- simulated', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['origin_err'], label='Decision tree', color=color, lw=3) # thik line for original error
plt.plot(eps_err, df['marg_err'], label='_nolegend_', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_err'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_err'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
y_ticks = eps_err[:]
y_ticks.extend([0.3, 0.4, 0.5])
plt.yticks(y_ticks)
plt.grid(True)
plt.legend()
plt.title('Error rate')

"""Plot: oneC"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['marg_oneC'], label='SVM', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_oneC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_oneC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['marg_oneC'], label='Decision tree', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_oneC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_oneC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
plt.grid(True)
plt.legend()
plt.title('oneC')

"""Plot: avgC"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['marg_avgC'], label='SVM', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_avgC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_avgC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['marg_avgC'], label='Decision tree', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_avgC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_avgC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
plt.grid(True)
plt.legend()
plt.title('avgC')

"""### car"""

from sklearn.preprocessing import LabelEncoder
def load_cars():
    print('Loading cars')
    # load cars dataset
    file = path_to_files +'datasets/cars.csv'
    df = pd.read_csv(file,header=None, sep=',')
#     df.drop(0, axis=0,inplace=True)
    target_names = df[df.columns[-1]].unique()
    target_names.sort()
    data_points = df[df.columns[:-1]]
    target = df[df.columns[-1]].values 
    label_encoder = LabelEncoder()
   
    target = label_encoder.fit_transform(target)
    for i in range(6):
        data_points.iloc[:,i] = label_encoder.fit_transform(data_points.iloc[:,i])
    
#     data_points = data_points.apply(lambda x:label_encoder.fit_transform(x),axis=1)
  
    # scale target 
#     target -= min(target)
    data = {'target': target, 'data': data_points.values,
            'target_names': target_names}
    
    return data

cars_df = pd.read_csv(path_to_files +'datasets/cars.csv',header=None,sep=',')
# print(len(cars_df))
# balance_df.drop(0, axis=0,inplace=True)
# print(len(cars_df))
cars_df.head()

data = load_cars()

def plotClasses(df):
    cols = ["col"+str(x) for x in range(1,len(df.columns))]
    cols.append("target")
    df.columns = cols # add column names to the data frame 
    counts = df.groupby(by="target").count()[['col1']]
    labels = counts.index.to_list()
    data_points = counts.values.reshape(len(counts),)
    return plt.bar(labels,data_points)

plotClasses(cars_df)

"""### Run conformal prediction

##### 1- svm
"""

svm_mean_df = analysis_with_2_func(data=data, model_obj=svm_model_obj, 
                     params=svm_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
svm_mean_df

"""##### 1-svm"""

svm_mean_df = analysis_with_2_func(data=data, model_obj=svm_model_obj, 
                     params=svm_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
svm_mean_df

"""##### decision tree"""

dt_mean_df = analysis_with_2_func(data=data, model_obj=dt_model_obj, 
                     params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))}, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
dt_mean_df

"""##### 3 - KNeighbors"""

knn_mean_df = analysis_with_2_func(data=data, model_obj=k_kneighbors_model_obj, 
                     params=k_kneighbors_params, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
knn_mean_df

"""##### 4 - AdaBoostClassifier"""

ada_boost_mean_df = analysis_with_2_func(data=data, model_obj=ada_boost_model_obj, 
                     params=ada_boost_params, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
ada_boost_mean_df

"""##### 5 - GaussianNB"""

gussian_nb_mean_df = analysis_with_2_func(data=data,model_obj=gaussian_nb_model_obj, 
                     params=gaussian_nb_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
gussian_nb_mean_df

"""##### 6 - MLPClassifier"""

mlp_classifier_mean_df = analysis_with_2_func(data=data, model_obj=mlp_classifier_model_obj, 
                     params=mlp_classifier_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
mlp_classifier_mean_df

"""##### 7 - RandomForestClassifier"""

random_forest_params[MIN_SAMPLES_KEY] = max(5, int(dt_min_sample_ratio * len(data['data'])))
random_forest_params

random_forest_mean_df = analysis_with_2_func(data=data, model_obj=random_forest_model_obj, 
                     params=random_forest_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
random_forest_mean_df

"""##### 8 - GaussianProcessClassifier"""

gaussian_process_mean_df = pd.DataFrame()

gaussian_process_mean_df = analysis_with_2_func(data=data, model_obj=gaussian_process_obj, 
                     params=gaussian_process_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
gaussian_process_mean_df

"""##### 9 - QuadraticDiscriminantAnalysis"""

quadratic_discriminant_mean_df = analysis_with_2_func(data=data, model_obj=quadratic_discriminant_model_obj, 
                     params=quadratic_discriminant_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
quadratic_discriminant_mean_df

"""### Plot results """

# give names to dataFrames
svm_mean_df.name = 'SVM'
dt_mean_df.name = 'DecisionTree'
knn_mean_df.name = 'KNN'
ada_boost_mean_df.name = 'AdaBoost'
gussian_nb_mean_df.name = 'GaussianNB'
mlp_classifier_mean_df.name = 'Multilayer Perceptron'
random_forest_mean_df.name = 'Random Forest'
gaussian_process_mean_df.name = 'Gaussian process'
quadratic_discriminant_mean_df.name = 'Quadratic discriminant analysis'

all_results = [svm_mean_df, dt_mean_df, knn_mean_df, ada_boost_mean_df, 
               gussian_nb_mean_df, mlp_classifier_mean_df, random_forest_mean_df,
               gaussian_process_mean_df, quadratic_discriminant_mean_df
               ]

"""##### error rate"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=['origin_err', 'marg_err', 'inv_err', 'Simul_err'], 
          title_str='Error rate', to_extend=[0.3, 0.4, 0.5])

"""##### oneC"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=[None, 'marg_oneC', 'inv_oneC', 'Simul_oneC'], 
          title_str='oneC')#, to_extend=[0.3, 0.4, 0.5])

"""##### avgC"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=[None, 'marg_avgC', 'inv_avgC', 'Simul_avgC'], 
          title_str='avgC')#, to_extend=[0.3, 0.4, 0.5])

"""<font color='red'>Results: Which non-conformity function is better for which metric (oneC should be hight, avgC shold be low)</font>

|   |                        | avgC      | oneC      |
|---|---                     |---        |---        |
| 1 | SVM                    | inv_prob  | margin    |
| 2 | Decision trees         | inv_prob  | margin    |
| 3 | KNN                    | inv_prob  | margin    |
| 4 | AdaBoost               | -----     | ------    |
| 5 | GaussianNB             | inv_prob  | margin    |
| 6 | Multilayer Perceptron  | inv_prob  | margin    |
| 7 | Random Forest          | inv_prob  | margin    |
| 8 | Gaussian process       | inv_prob  | margin    |
| 9 | Quadratic discriminant analysis  | inv_prob  | margin    |

### svm

##### MarginErrFunc
"""

res_df = run_conformal(data, epsilon_arr=eps_err, err_func=MarginErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
res_df.groupby('eps').mean()

"""### InverseProbabilityErrFunc"""

res_df = run_conformal(data, epsilon_arr=eps_err, err_func=InverseProbabilityErrFunc, smoothing=True,
                             seed=1, verbose=True)
# Get average values of metrics
print('average values for each eps')
res_df.groupby('eps').mean()

tmp_df = res_df.groupby('eps').mean()
# rename columns for margin function
svm_mean_df.columns = ['fold', 'origin_err', 'marg_err', 'marg_oneC', 'marg_avgC', 
                       'Simul_err', 'Simul_oneC', 'Simul_avgC','inv_err','inv_oneC','inv_avgC']
# these columns contain results for inverse probability non-conformity function
svm_mean_df['inv_err'] = tmp_df['Conf_err']
svm_mean_df['inv_oneC'] = tmp_df['Conf_oneC']
svm_mean_df['inv_avgC'] = tmp_df['Conf_avgC']
svm_mean_df

"""### decision tree

### MarginErrFunc
"""

res_df = run_conformal(data, model_obj=dt_model_obj, 
                       params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))},
                       epsilon_arr=eps_err, err_func=MarginErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
dt_mean_df = res_df.groupby('eps').mean()
dt_mean_df

"""### InverseProbabilityErrFunc"""

res_df = run_conformal(data, model_obj=dt_model_obj, 
                       params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))},
                       epsilon_arr=eps_err, err_func=InverseProbabilityErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
res_df.groupby('eps').mean()

tmp_df = res_df.groupby('eps').mean()
# rename columns for margin function
dt_mean_df.columns = ['fold', 'origin_err', 'marg_err', 'marg_oneC', 'marg_avgC', 
                       'Simul_err', 'Simul_oneC', 'Simul_avgC']
# these columns contain results for inverse probability non-conformity function
dt_mean_df['inv_err'] = tmp_df['Conf_err']
dt_mean_df['inv_oneC'] = tmp_df['Conf_oneC']
dt_mean_df['inv_avgC'] = tmp_df['Conf_avgC']
dt_mean_df

"""### Plot results: error rate"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['origin_err'], label='SVM', color=color, lw=3) # thik line for original error
plt.plot(eps_err, df['marg_err'], label='- margin', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_err'], label='- inv prob', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_err'], label='- simulated', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['origin_err'], label='Decision tree', color=color, lw=3) # thik line for original error
plt.plot(eps_err, df['marg_err'], label='_nolegend_', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_err'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_err'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
y_ticks = eps_err[:]
y_ticks.extend([0.3, 0.4, 0.5])
plt.yticks(y_ticks)
plt.grid(True)
plt.legend()
plt.title('Error rate')

"""### Plot: oneC"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['marg_oneC'], label='SVM', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_oneC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_oneC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['marg_oneC'], label='Decision tree', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_oneC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_oneC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
plt.grid(True)
plt.legend()
plt.title('oneC')

"""### avgC"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['marg_avgC'], label='SVM', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_avgC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_avgC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['marg_avgC'], label='Decision tree', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_avgC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_avgC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
plt.grid(True)
plt.legend()
plt.title('avgC')

"""### heat"""

from sklearn.preprocessing import LabelEncoder
def load_heat():
    print('Loading heat')
    # load heat dataset
    file = path_to_files +'datasets/heat.csv'
    df = pd.read_csv(file,header=0, sep=',')
    target_names = df.iloc[:,-1].unique()
    target_names.sort()
    label_encoder = LabelEncoder()
    data_points = df.iloc[:,2:-1] 
    target = df[df.columns[-1]].values 
    
    # scale target 
#     target -= min(target)
    data = {'target': target, 'data': data_points.values,
            'target_names': target_names}
    return data

heat_df = pd.read_csv(path_to_files + 'datasets/heat.csv',header=0,sep=',')
print(heat_df.shape)
# balance_df.drop(0, axis=1,inplace=True)
heat_df.head()
# heat_df.iloc[:,-1].unique()

data = load_heat()

def plotClasses(df):
    cols = ["col"+str(x) for x in range(1,len(df.columns))]
    cols.append("target")
    df.columns = cols # add column names to the data frame 
    counts = df.groupby(by="target").count()[['col1']]
    labels = counts.index.to_list()
    data_points = counts.values.reshape(len(counts),)
    return plt.bar(labels,data_points)

plotClasses(heat_df)

"""### Run conformal prediction

##### 1- Svm
"""

svm_mean_df = analysis_with_2_func(data=data, model_obj=svm_model_obj, 
                     params=svm_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
svm_mean_df

"""##### decision tree"""

dt_mean_df = analysis_with_2_func(data=data, model_obj=dt_model_obj, 
                     params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))}, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
dt_mean_df

"""##### 3 - KNeighbors"""

knn_mean_df = analysis_with_2_func(data=data, model_obj=k_kneighbors_model_obj, 
                     params=k_kneighbors_params, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
knn_mean_df

"""##### 4 - AdaBoostClassifier"""

ada_boost_mean_df = analysis_with_2_func(data=data, model_obj=ada_boost_model_obj, 
                     params=ada_boost_params, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
ada_boost_mean_df

"""##### 5- GaussianNB"""

gussian_nb_mean_df = analysis_with_2_func(data=data,model_obj=gaussian_nb_model_obj, 
                     params=gaussian_nb_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
gussian_nb_mean_df

"""##### 6 - MLPClassifier"""

mlp_classifier_mean_df = analysis_with_2_func(data=data, model_obj=mlp_classifier_model_obj, 
                     params=mlp_classifier_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
mlp_classifier_mean_df

"""##### 7 - RandomForestClassifier"""

random_forest_params[MIN_SAMPLES_KEY] = max(5, int(dt_min_sample_ratio * len(data['data'])))
random_forest_params

random_forest_mean_df = analysis_with_2_func(data=data, model_obj=random_forest_model_obj, 
                     params=random_forest_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
random_forest_mean_df

"""##### 8 - GaussianProcessClassifier"""

gaussian_process_mean_df = pd.DataFrame()

gaussian_process_mean_df = analysis_with_2_func(data=data, model_obj=gaussian_process_obj, 
                     params=gaussian_process_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
gaussian_process_mean_df

"""##### 9 - QuadraticDiscriminantAnalysis"""

quadratic_discriminant_mean_df = analysis_with_2_func(data=data, model_obj=quadratic_discriminant_model_obj, 
                     params=quadratic_discriminant_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
quadratic_discriminant_mean_df

"""### Plot results"""

# give names to dataFrames
svm_mean_df.name = 'SVM'
dt_mean_df.name = 'DecisionTree'
knn_mean_df.name = 'KNN'
ada_boost_mean_df.name = 'AdaBoost'
gussian_nb_mean_df.name = 'GaussianNB'
mlp_classifier_mean_df.name = 'Multilayer Perceptron'
random_forest_mean_df.name = 'Random Forest'
gaussian_process_mean_df.name = 'Gaussian process'
quadratic_discriminant_mean_df.name = 'Quadratic discriminant analysis'

all_results = [svm_mean_df, dt_mean_df, knn_mean_df, ada_boost_mean_df, 
               gussian_nb_mean_df, mlp_classifier_mean_df, random_forest_mean_df,
               gaussian_process_mean_df, quadratic_discriminant_mean_df
               ]

"""#### error rate"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=['origin_err', 'marg_err', 'inv_err', 'Simul_err'], 
          title_str='Error rate', to_extend=[0.3, 0.4, 0.5])

"""#### oneC"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=[None, 'marg_oneC', 'inv_oneC', 'Simul_oneC'], 
          title_str='oneC')#, to_extend=[0.3, 0.4, 0.5])

"""#### aavgC"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=[None, 'marg_avgC', 'inv_avgC', 'Simul_avgC'], 
          title_str='avgC')#, to_extend=[0.3, 0.4, 0.5])

"""<font color='red'>Results: Which non-conformity function is better for which metric (oneC should be hight, avgC shold be low)</font>

|   |                        | avgC      | oneC      |
|---|---                     |---        |---        |
| 1 | SVM                    | inv_prob  | margin    |
| 2 | Decision trees         | inv_prob  | margin    |
| 3 | KNN                    | inv_prob  | margin    |
| 4 | AdaBoost               | -----     | ------    |
| 5 | GaussianNB             | inv_prob  | margin    |
| 6 | Multilayer Perceptron  | inv_prob  | margin    |
| 7 | Random Forest          | inv_prob  | margin    |
| 8 | Gaussian process       | inv_prob  | margin    |
| 9 | Quadratic discriminant analysis  | inv_prob  | margin    |

### svm

### MarginErrFunc
"""

res_df = run_conformal(data, epsilon_arr=eps_err, err_func=MarginErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
res_df.groupby('eps').mean()

"""### InverseProbabilityErrFunc"""

res_df = run_conformal(data, epsilon_arr=eps_err, err_func=InverseProbabilityErrFunc, smoothing=True,
                             seed=1, verbose=True)
# Get average values of metrics
print('average values for each eps')
res_df.groupby('eps').mean()

tmp_df = res_df.groupby('eps').mean()
# rename columns for margin function
svm_mean_df.columns = ['fold', 'origin_err', 'marg_err', 'marg_oneC', 'marg_avgC', 
                       'Simul_err', 'Simul_oneC', 'Simul_avgC','inv_err','inv_oneC','inv_avgC']
# these columns contain results for inverse probability non-conformity function
svm_mean_df['inv_err'] = tmp_df['Conf_err']
svm_mean_df['inv_oneC'] = tmp_df['Conf_oneC']
svm_mean_df['inv_avgC'] = tmp_df['Conf_avgC']
svm_mean_df

"""#### decision tree

### MarginErrFunc
"""

res_df = run_conformal(data, model_obj=dt_model_obj, 
                       params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))},
                       epsilon_arr=eps_err, err_func=MarginErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
dt_mean_df = res_df.groupby('eps').mean()
dt_mean_df

"""### InverseProbabilityErrFunc"""

res_df = run_conformal(data, model_obj=dt_model_obj, 
                       params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))},
                       epsilon_arr=eps_err, err_func=InverseProbabilityErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
res_df.groupby('eps').mean()

tmp_df = res_df.groupby('eps').mean()
# rename columns for margin function
dt_mean_df.columns = ['fold', 'origin_err', 'marg_err', 'marg_oneC', 'marg_avgC', 
                       'Simul_err', 'Simul_oneC', 'Simul_avgC']
# these columns contain results for inverse probability non-conformity function
dt_mean_df['inv_err'] = tmp_df['Conf_err']
dt_mean_df['inv_oneC'] = tmp_df['Conf_oneC']
dt_mean_df['inv_avgC'] = tmp_df['Conf_avgC']
dt_mean_df

"""### Plot results: error rate"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['origin_err'], label='SVM', color=color, lw=3) # thik line for original error
plt.plot(eps_err, df['marg_err'], label='- margin', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_err'], label='- inv prob', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_err'], label='- simulated', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['origin_err'], label='Decision tree', color=color, lw=3) # thik line for original error
plt.plot(eps_err, df['marg_err'], label='_nolegend_', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_err'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_err'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
y_ticks = eps_err[:]
y_ticks.extend([0.3, 0.4, 0.5])
plt.yticks(y_ticks)
plt.grid(True)
plt.legend()
plt.title('Error rate')

"""### Plot: oneC


"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['marg_oneC'], label='SVM', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_oneC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_oneC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['marg_oneC'], label='Decision tree', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_oneC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_oneC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
plt.grid(True)
plt.legend()
plt.title('oneC')

"""### avgC"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['marg_avgC'], label='SVM', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_avgC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_avgC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['marg_avgC'], label='Decision tree', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_avgC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_avgC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
plt.grid(True)
plt.legend()
plt.title('avgC')

"""### wine"""

from sklearn.preprocessing import LabelEncoder
def load_wine():
    print('Loading wine')
    # load wne dataset
    file = path_to_files +'datasets/wine.csv'
    df = pd.read_csv(file,header=None, sep=',')
#     df.drop(0, axis=1,inplace=True)
    target_names = df.iloc[:,0].unique()
    target_names.sort()
    
    target = df.iloc[:,0].values 
    
    # scale target 
#     target -= min(target)
    data = {'target': target, 'data': df.iloc[:,1:-1].values,
            'target_names': target_names}
    return data

wine_df = pd.read_csv(path_to_files +'datasets/wine.csv',header=None,sep=',')
# print(len(wine_df))
# balance_df.drop(0, axis=1,inplace=True)
wine_df.head()
wine_df.iloc[:,0].unique()

data = load_wine()

def plotClasses(df):
    cols = ["col"+str(x) for x in range(1,len(df.columns))]
    cols.insert(0,"target")
    df.columns = cols # add column names to the data frame 
    counts = df.groupby(by="target").count()[['col1']]
    labels = counts.index.to_list()
    data_points = counts.values.reshape(len(counts),)
    return plt.bar(labels,data_points)

plotClasses(wine_df)

"""### Run conformal prediction

##### 1- svm
"""

svm_mean_df = analysis_with_2_func(data=data, model_obj=svm_model_obj, 
                     params=svm_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
svm_mean_df

"""##### 2- decision tree"""

dt_mean_df = analysis_with_2_func(data=data, model_obj=dt_model_obj, 
                     params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))}, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
dt_mean_df

"""##### 3 - KNeighbors"""

knn_mean_df = analysis_with_2_func(data=data, model_obj=k_kneighbors_model_obj, 
                     params=k_kneighbors_params, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
knn_mean_df

"""##### 4 - AdaBoostClassifier"""

ada_boost_mean_df = analysis_with_2_func(data=data, model_obj=ada_boost_model_obj, 
                     params=ada_boost_params, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
ada_boost_mean_df

"""##### 5 - GaussianNB"""

gussian_nb_mean_df = analysis_with_2_func(data=data,model_obj=gaussian_nb_model_obj, 
                     params=gaussian_nb_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
gussian_nb_mean_df

"""##### 6 - MLPClassifier"""

mlp_classifier_mean_df = analysis_with_2_func(data=data, model_obj=mlp_classifier_model_obj, 
                     params=mlp_classifier_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
mlp_classifier_mean_df

"""##### 7 - RandomForestClassifier"""

random_forest_params[MIN_SAMPLES_KEY] = max(5, int(dt_min_sample_ratio * len(data['data'])))
random_forest_params

random_forest_mean_df = analysis_with_2_func(data=data, model_obj=random_forest_model_obj, 
                     params=random_forest_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
random_forest_mean_df

"""##### 8 - GaussianProcessClassifier"""

gaussian_process_mean_df = pd.DataFrame()

gaussian_process_mean_df = analysis_with_2_func(data=data, model_obj=gaussian_process_obj, 
                     params=gaussian_process_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
gaussian_process_mean_df

"""9 - QuadraticDiscriminantAnalysis"""

quadratic_discriminant_mean_df = analysis_with_2_func(data=data, model_obj=quadratic_discriminant_model_obj, 
                     params=quadratic_discriminant_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
quadratic_discriminant_mean_df

"""### Plot results"""

# give names to dataFrames
svm_mean_df.name = 'SVM'
dt_mean_df.name = 'DecisionTree'
knn_mean_df.name = 'KNN'
ada_boost_mean_df.name = 'AdaBoost'
gussian_nb_mean_df.name = 'GaussianNB'
mlp_classifier_mean_df.name = 'Multilayer Perceptron'
random_forest_mean_df.name = 'Random Forest'
gaussian_process_mean_df.name = 'Gaussian process'
quadratic_discriminant_mean_df.name = 'Quadratic discriminant analysis'

all_results = [svm_mean_df, dt_mean_df, knn_mean_df, ada_boost_mean_df, 
               gussian_nb_mean_df, mlp_classifier_mean_df, random_forest_mean_df,
               gaussian_process_mean_df, quadratic_discriminant_mean_df
               ]

"""#### Error rate"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=['origin_err', 'marg_err', 'inv_err', 'Simul_err'], 
          title_str='Error rate', to_extend=[0.3, 0.4, 0.5])

"""#### oneC"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=[None, 'marg_oneC', 'inv_oneC', 'Simul_oneC'], 
          title_str='oneC')#, to_extend=[0.3, 0.4, 0.5])

"""#### avgC"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=[None, 'marg_avgC', 'inv_avgC', 'Simul_avgC'], 
          title_str='avgC')#, to_extend=[0.3, 0.4, 0.5])

"""<font color='red'>Results: Which non-conformity function is better for which metric (oneC should be hight, avgC shold be low)</font>

|   |                        | avgC      | oneC      |
|---|---                     |---        |---        |
| 1 | SVM                    | inv_prob  | margin    |
| 2 | Decision trees         | inv_prob  | margin    |
| 3 | KNN                    | inv_prob  | margin    |
| 4 | AdaBoost               | -----     | ------    |
| 5 | GaussianNB             | inv_prob  | margin    |
| 6 | Multilayer Perceptron  | inv_prob  | margin    |
| 7 | Random Forest          | inv_prob  | margin    |
| 8 | Gaussian process       | inv_prob  | margin    |
| 9 | Quadratic discriminant analysis  | inv_prob  | margin    |

### svm

### MarginErrFunc
"""

res_df = run_conformal(data, epsilon_arr=eps_err, err_func=MarginErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
res_df.groupby('eps').mean()



"""### InverseProbabilityErrFunc"""

res_df = run_conformal(data, epsilon_arr=eps_err, err_func=InverseProbabilityErrFunc, smoothing=True,
                             seed=1, verbose=True)
# Get average values of metrics
print('average values for each eps')
res_df.groupby('eps').mean()

tmp_df = res_df.groupby('eps').mean()
# rename columns for margin function
dt_mean_df.columns = ['fold', 'origin_err', 'marg_err', 'marg_oneC', 'marg_avgC', 
                       'Simul_err', 'Simul_oneC', 'Simul_avgC','inv_err','inv_oneC','inc_avgC']
# these columns contain results for inverse probability non-conformity function
dt_mean_df['inv_err'] = tmp_df['Conf_err']
dt_mean_df['inv_oneC'] = tmp_df['Conf_oneC']
dt_mean_df['inv_avgC'] = tmp_df['Conf_avgC']
dt_mean_df

"""### decision tree

### MarginErrFunc
"""

res_df = run_conformal(data, model_obj=dt_model_obj, 
                       params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))},
                       epsilon_arr=eps_err, err_func=MarginErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
dt_mean_df = res_df.groupby('eps').mean()
dt_mean_df



"""### InverseProbabilityErrFunc"""

res_df = run_conformal(data, model_obj=dt_model_obj, 
                       params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))},
                       epsilon_arr=eps_err, err_func=InverseProbabilityErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
res_df.groupby('eps').mean()

tmp_df = res_df.groupby('eps').mean()
# rename columns for margin function
dt_mean_df.columns = ['fold', 'origin_err', 'marg_err', 'marg_oneC', 'marg_avgC', 
                       'Simul_err', 'Simul_oneC', 'Simul_avgC']
# these columns contain results for inverse probability non-conformity function
dt_mean_df['inv_err'] = tmp_df['Conf_err']
dt_mean_df['inv_oneC'] = tmp_df['Conf_oneC']
dt_mean_df['inv_avgC'] = tmp_df['Conf_avgC']
dt_mean_df



"""## Plot results: error rate"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['origin_err'], label='SVM', color=color, lw=3) # thik line for original error
plt.plot(eps_err, df['marg_err'], label='- margin', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_err'], label='- inv prob', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_err'], label='- simulated', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['origin_err'], label='Decision tree', color=color, lw=3) # thik line for original error
plt.plot(eps_err, df['marg_err'], label='_nolegend_', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_err'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_err'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
y_ticks = eps_err[:]
y_ticks.extend([0.3, 0.4, 0.5])
plt.yticks(y_ticks)
plt.grid(True)
plt.legend()
plt.title('Error rate')

"""Plot: oneC"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['marg_oneC'], label='SVM', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_oneC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_oneC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['marg_oneC'], label='Decision tree', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_oneC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_oneC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
plt.grid(True)
plt.legend()
plt.title('oneC')

"""avgC"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['marg_avgC'], label='SVM', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_avgC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_avgC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['marg_avgC'], label='Decision tree', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_avgC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_avgC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
plt.grid(True)
plt.legend()

"""### steel"""

from sklearn.preprocessing import LabelEncoder
def load_steel():
    print('Loading steel')
    # load steel dataset
    file = path_to_files+'datasets/steel.csv'
    df = pd.read_csv(file,header=None, sep=',')
    df.drop(0, axis=1,inplace=True)
    target_names = df[df.columns[-1]].unique()
    target_names.sort()
    
    target = df[df.columns[-1]].values 
    
    # scale target 
    target -= min(target)
    data = {'target': target, 'data': df[df.columns[:-1]].values,
            'target_names': target_names}
    return data

steel_df = pd.read_csv(path_to_files +'datasets/steel.csv',header=None,sep=',')
# print(len(steel_df))
# steel_df.drop(0, axis=1,inplace=True)
steel_df.head()

data = load_steel()

def plotClasses(df):
    cols = ["col"+str(x) for x in range(1,len(df.columns))]
    cols.append("target")
    df.columns = cols # add column names to the data frame 
    counts = df.groupby(by="target").count()[['col1']]
    labels = counts.index.to_list()
    data_points = counts.values.reshape(len(counts),)
    return plt.bar(labels,data_points)

plotClasses(steel_df)

"""### Run conformal prediction

##### 1- svm
"""

svm_mean_df = analysis_with_2_func(data=data, model_obj=svm_model_obj, 
                     params=svm_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
svm_mean_df

"""##### 2-decision tree"""

dt_mean_df = analysis_with_2_func(data=data, model_obj=dt_model_obj, 
                     params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))}, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
dt_mean_df

"""##### 3 - KNeighbors"""

knn_mean_df = analysis_with_2_func(data=data, model_obj=k_kneighbors_model_obj, 
                     params=k_kneighbors_params, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
knn_mean_df

"""##### 4 - AdaBoostClassifier"""

ada_boost_mean_df = analysis_with_2_func(data=data, model_obj=ada_boost_model_obj, 
                     params=ada_boost_params, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
ada_boost_mean_df

"""##### 5 - GaussianNB"""

gussian_nb_mean_df = analysis_with_2_func(data=data,model_obj=gaussian_nb_model_obj, 
                     params=gaussian_nb_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
gussian_nb_mean_df

"""##### 6 - MLPClassifier"""

mlp_classifier_mean_df = analysis_with_2_func(data=data, model_obj=mlp_classifier_model_obj, 
                     params=mlp_classifier_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
mlp_classifier_mean_df

"""##### 7 - RandomForestClassifier"""

random_forest_params[MIN_SAMPLES_KEY] = max(5, int(dt_min_sample_ratio * len(data['data'])))
random_forest_params

random_forest_mean_df = analysis_with_2_func(data=data, model_obj=random_forest_model_obj, 
                     params=random_forest_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
random_forest_mean_df

"""##### 8 - GaussianProcessClassifier"""

gaussian_process_mean_df = pd.DataFrame()

gaussian_process_mean_df = analysis_with_2_func(data=data, model_obj=gaussian_process_obj, 
                     params=gaussian_process_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
gaussian_process_mean_df

"""##### 9 - QuadraticDiscriminantAnalysis"""

quadratic_discriminant_mean_df = analysis_with_2_func(data=data, model_obj=quadratic_discriminant_model_obj, 
                     params=quadratic_discriminant_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
quadratic_discriminant_mean_df

"""### Plot results"""

# give names to dataFrames
svm_mean_df.name = 'SVM'
dt_mean_df.name = 'DecisionTree'
knn_mean_df.name = 'KNN'
ada_boost_mean_df.name = 'AdaBoost'
gussian_nb_mean_df.name = 'GaussianNB'
mlp_classifier_mean_df.name = 'Multilayer Perceptron'
random_forest_mean_df.name = 'Random Forest'
gaussian_process_mean_df.name = 'Gaussian process'
quadratic_discriminant_mean_df.name = 'Quadratic discriminant analysis'

all_results = [svm_mean_df, dt_mean_df, knn_mean_df, ada_boost_mean_df, 
               gussian_nb_mean_df, mlp_classifier_mean_df, random_forest_mean_df,
               gaussian_process_mean_df, quadratic_discriminant_mean_df
               ]

"""### Error rate"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=['origin_err', 'marg_err', 'inv_err', 'Simul_err'], 
          title_str='Error rate', to_extend=[0.3, 0.4, 0.5])

"""### oneC"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=[None, 'marg_oneC', 'inv_oneC', 'Simul_oneC'], 
          title_str='oneC')#, to_extend=[0.3, 0.4, 0.5])

"""### avgC"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=[None, 'marg_avgC', 'inv_avgC', 'Simul_avgC'], 
          title_str='avgC')#, to_extend=[0.3, 0.4, 0.5])

"""<font color='red'>Results: Which non-conformity function is better for which metric (oneC should be hight, avgC shold be low)</font>

|   |                        | avgC      | oneC      |
|---|---                     |---        |---        |
| 1 | SVM                    | inv_prob  | margin    |
| 2 | Decision trees         | inv_prob  | margin    |
| 3 | KNN                    | inv_prob  | margin    |
| 4 | AdaBoost               | -----     | ------    |
| 5 | GaussianNB             | inv_prob  | margin    |
| 6 | Multilayer Perceptron  | inv_prob  | margin    |
| 7 | Random Forest          | inv_prob  | margin    |
| 8 | Gaussian process       | inv_prob  | margin    |
| 9 | Quadratic discriminant analysis  | inv_prob  | margin    |

### svm
"""



"""### MarginErrFunc"""

res_df = run_conformal(data, model_obj=svm_model_obj, 
                       params=svm_params,
                       epsilon_arr=eps_err, err_func=MarginErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
svm_mean_df = res_df.groupby('eps').mean()
svm_mean_df

"""### InverseProbabilityErrFunc"""

res_df = run_conformal(data, epsilon_arr=eps_err, err_func=InverseProbabilityErrFunc, smoothing=True,
                             seed=1, verbose=True)
# Get average values of metrics
print('average values for each eps')
res_df.groupby('eps').mean()

tmp_df = res_df.groupby('eps').mean()
# rename columns for margin function
svm_mean_df.columns = ['fold', 'origin_err', 'marg_err', 'marg_oneC', 'marg_avgC', 
                       'Simul_err', 'Simul_oneC', 'Simul_avgC']
# these columns contain results for inverse probability non-conformity function
svm_mean_df['inv_err'] = tmp_df['Conf_err']
svm_mean_df['inv_oneC'] = tmp_df['Conf_oneC']
svm_mean_df['inv_avgC'] = tmp_df['Conf_avgC']
svm_mean_df

"""### decision tree"""



"""### MarginErrFunc"""

res_df = run_conformal(data, model_obj=dt_model_obj, 
                       params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))},
                       epsilon_arr=eps_err, err_func=MarginErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
dt_mean_df = res_df.groupby('eps').mean()
dt_mean_df

"""### InverseProbabilityErrFunc"""

res_df = run_conformal(data, model_obj=dt_model_obj, 
                       params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))},
                       epsilon_arr=eps_err, err_func=InverseProbabilityErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
res_df.groupby('eps').mean()

tmp_df = res_df.groupby('eps').mean()
# rename columns for margin function
dt_mean_df.columns = ['fold', 'origin_err', 'marg_err', 'marg_oneC', 'marg_avgC', 
                       'Simul_err', 'Simul_oneC', 'Simul_avgC']
# these columns contain results for inverse probability non-conformity function
dt_mean_df['inv_err'] = tmp_df['Conf_err']
dt_mean_df['inv_oneC'] = tmp_df['Conf_oneC']
dt_mean_df['inv_avgC'] = tmp_df['Conf_avgC']
dt_mean_df

"""### Plot results: error rate"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['origin_err'], label='SVM', color=color, lw=3) # thik line for original error
plt.plot(eps_err, df['marg_err'], label='- margin', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_err'], label='- inv prob', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_err'], label='- simulated', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['origin_err'], label='Decision tree', color=color, lw=3) # thik line for original error
plt.plot(eps_err, df['marg_err'], label='_nolegend_', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_err'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_err'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
y_ticks = eps_err[:]
y_ticks.extend([0.3, 0.4, 0.5])
plt.yticks(y_ticks)
plt.grid(True)
plt.legend()
plt.title('Error rate')

"""### oneC"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['marg_oneC'], label='SVM', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_oneC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_oneC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['marg_oneC'], label='Decision tree', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_oneC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_oneC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
plt.grid(True)
plt.legend()
plt.title('oneC')

"""### avgC"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['marg_avgC'], label='SVM', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_avgC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_avgC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['marg_avgC'], label='Decision tree', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_avgC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_avgC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
plt.grid(True)
plt.legend()
plt.title('avgC')

"""### user

"""

from sklearn.preprocessing import LabelEncoder
def load_user():
    print('Loading user')
    # load user dataset
    file = path_to_files+'datasets/user.csv'
    df = pd.read_csv(file,header=0, sep=',')
    df.dropna( axis=1,inplace=True)
    target_names = df.iloc[:,-1].unique()
    target_names.sort()
    target = df.iloc[:,-1].values
    label_encoder = LabelEncoder()
    target = label_encoder.fit_transform(target)
    data_points = df.iloc[:,0:-1]
    # scale target 
#     target -= min(target)
    data = {'target': target, 'data':data_points.values,
            'target_names': target_names}
    return data

user_df = pd.read_csv(path_to_files +'datasets/user.csv',header=0,sep=',')
print(len(user_df))
user_df.dropna( axis=1,inplace=True)
print(len(user_df))
user_df.head()

data = load_user()

def plotClasses(df):
    
    cols = ["col"+str(x) for x in range(1,len(df.columns))]
    cols.append("target")
    df.columns = cols # add column names to the data frame 
    counts = df.groupby(by="target").count()[['col1']]
    labels = counts.index.to_list()
    data_points = counts.values.reshape(len(counts),)
    return plt.bar(labels,data_points)

plotClasses(user_df)

"""### Run conformal prediction

##### 1-svm
"""

svm_mean_df = analysis_with_2_func(data=data, model_obj=svm_model_obj, 
                     params=svm_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
svm_mean_df

"""##### 2-decision tree"""

dt_mean_df = analysis_with_2_func(data=data, model_obj=dt_model_obj, 
                     params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))}, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
dt_mean_df

"""#### 3 - KNeighbors"""

knn_mean_df = analysis_with_2_func(data=data, model_obj=k_kneighbors_model_obj, 
                     params=k_kneighbors_params, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
knn_mean_df

"""#### 4 - AdaBoostClassifier"""

ada_boost_mean_df = analysis_with_2_func(data=data, model_obj=ada_boost_model_obj, 
                     params=ada_boost_params, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
ada_boost_mean_df

"""##### 5 - GaussianNB"""

gussian_nb_mean_df = analysis_with_2_func(data=data,model_obj=gaussian_nb_model_obj, 
                     params=gaussian_nb_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
gussian_nb_mean_df

"""##### 6 - MLPClassifier"""

mlp_classifier_mean_df = analysis_with_2_func(data=data, model_obj=mlp_classifier_model_obj, 
                     params=mlp_classifier_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
mlp_classifier_mean_df

"""##### 7 - RandomForestClassifier"""

random_forest_params[MIN_SAMPLES_KEY] = max(5, int(dt_min_sample_ratio * len(data['data'])))
random_forest_params

random_forest_mean_df = analysis_with_2_func(data=data, model_obj=random_forest_model_obj, 
                     params=random_forest_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
random_forest_mean_df

"""8 - GaussianProcessClassifier"""

gaussian_process_mean_df = pd.DataFrame()

gaussian_process_mean_df = analysis_with_2_func(data=data, model_obj=gaussian_process_obj, 
                     params=gaussian_process_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
gaussian_process_mean_df

"""#### 9 - QuadraticDiscriminantAnalysis"""

quadratic_discriminant_mean_df = analysis_with_2_func(data=data, model_obj=quadratic_discriminant_model_obj, 
                     params=quadratic_discriminant_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
quadratic_discriminant_mean_df

"""### Plot results"""

# give names to dataFrames
svm_mean_df.name = 'SVM'
dt_mean_df.name = 'DecisionTree'
knn_mean_df.name = 'KNN'
ada_boost_mean_df.name = 'AdaBoost'
gussian_nb_mean_df.name = 'GaussianNB'
mlp_classifier_mean_df.name = 'Multilayer Perceptron'
random_forest_mean_df.name = 'Random Forest'
gaussian_process_mean_df.name = 'Gaussian process'
quadratic_discriminant_mean_df.name = 'Quadratic discriminant analysis'

all_results = [svm_mean_df, dt_mean_df, knn_mean_df, ada_boost_mean_df, 
               gussian_nb_mean_df, mlp_classifier_mean_df, random_forest_mean_df,
               gaussian_process_mean_df, quadratic_discriminant_mean_df
               ]

"""### Error rate"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=['origin_err', 'marg_err', 'inv_err', 'Simul_err'], 
          title_str='Error rate', to_extend=[0.3, 0.4, 0.5])

"""### oneC"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=[None, 'marg_oneC', 'inv_oneC', 'Simul_oneC'], 
          title_str='oneC')#, to_extend=[0.3, 0.4, 0.5])

"""### avgC"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=[None, 'marg_avgC', 'inv_avgC', 'Simul_avgC'], 
          title_str='avgC')#, to_extend=[0.3, 0.4, 0.5])

"""<font color='red'>Results: Which non-conformity function is better for which metric (oneC should be hight, avgC shold be low)</font>

|   |                        | avgC      | oneC      |
|---|---                     |---        |---        |
| 1 | SVM                    | inv_prob  | margin    |
| 2 | Decision trees         | inv_prob  | margin    |
| 3 | KNN                    | inv_prob  | margin    |
| 4 | AdaBoost               | -----     | ------    |
| 5 | GaussianNB             | inv_prob  | margin    |
| 6 | Multilayer Perceptron  | inv_prob  | margin    |
| 7 | Random Forest          | inv_prob  | margin    |
| 8 | Gaussian process       | inv_prob  | margin    |
| 9 | Quadratic discriminant analysis  | inv_prob  | margin    |

### svm

### MarginErrFunc
"""

res_df = run_conformal(data, model_obj=svm_model_obj, 
                       params=svm_params,
                       epsilon_arr=eps_err, err_func=MarginErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
svm_mean_df = res_df.groupby('eps').mean()
svm_mean_df

"""### InverseProbabilityErrFunc"""

res_df = run_conformal(data, epsilon_arr=eps_err, err_func=InverseProbabilityErrFunc, smoothing=True,
                             seed=1, verbose=True)
# Get average values of metrics
print('average values for each eps')
res_df.groupby('eps').mean()

tmp_df = res_df.groupby('eps').mean()
# rename columns for margin function
svm_mean_df.columns = ['fold', 'origin_err', 'marg_err', 'marg_oneC', 'marg_avgC', 
                       'Simul_err', 'Simul_oneC', 'Simul_avgC']
# these columns contain results for inverse probability non-conformity function
svm_mean_df['inv_err'] = tmp_df['Conf_err']
svm_mean_df['inv_oneC'] = tmp_df['Conf_oneC']
svm_mean_df['inv_avgC'] = tmp_df['Conf_avgC']
svm_mean_df

"""### decision tree"""



"""### MarginErrFunc"""

res_df = run_conformal(data, model_obj=dt_model_obj, 
                       params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))},
                       epsilon_arr=eps_err, err_func=MarginErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
dt_mean_df = res_df.groupby('eps').mean()
dt_mean_df

"""###InverseProbabilityErrFunc"""

res_df = run_conformal(data, model_obj=dt_model_obj, 
                       params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))},
                       epsilon_arr=eps_err, err_func=InverseProbabilityErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
res_df.groupby('eps').mean()

tmp_df = res_df.groupby('eps').mean()
# rename columns for margin function
dt_mean_df.columns = ['fold', 'origin_err', 'marg_err', 'marg_oneC', 'marg_avgC', 
                       'Simul_err', 'Simul_oneC', 'Simul_avgC']
# these columns contain results for inverse probability non-conformity function
dt_mean_df['inv_err'] = tmp_df['Conf_err']
dt_mean_df['inv_oneC'] = tmp_df['Conf_oneC']
dt_mean_df['inv_avgC'] = tmp_df['Conf_avgC']
dt_mean_df

"""Plot results: error rate"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['origin_err'], label='SVM', color=color, lw=3) # thik line for original error
plt.plot(eps_err, df['marg_err'], label='- margin', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_err'], label='- inv prob', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_err'], label='- simulated', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['origin_err'], label='Decision tree', color=color, lw=3) # thik line for original error
plt.plot(eps_err, df['marg_err'], label='_nolegend_', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_err'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_err'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
y_ticks = eps_err[:]
y_ticks.extend([0.3, 0.4, 0.5])
plt.yticks(y_ticks)
plt.grid(True)
plt.legend()
plt.title('Error rate')

"""### oneC"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['marg_oneC'], label='SVM', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_oneC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_oneC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['marg_oneC'], label='Decision tree', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_oneC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_oneC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
plt.grid(True)
plt.legend()
plt.title('oneC')

"""### avgC"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['marg_avgC'], label='SVM', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_avgC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_avgC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['marg_avgC'], label='Decision tree', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_avgC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_avgC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
plt.grid(True)
plt.legend()
plt.title('avgC')

"""###wave"""

from sklearn.preprocessing import LabelEncoder
def load_wave():
    print('Loading wave')
    # load wave dataset
    file = path_to_files+'datasets/wave.csv'
    df = pd.read_csv(file,header=None, sep=',')
    df.drop(0, axis=1,inplace=True)
    target_names = df.iloc[:,-1].unique()
    target_names.sort()
    target = df.iloc[:,-1].values 
    # scale target 
    target -= min(target)
    data = {'target': target, 'data': df.iloc[:,0:-1].values,
            'target_names': target_names}
    return data

wave_df = pd.read_csv(path_to_files +'datasets/wave.csv',header=None,sep=',')
# print(len(wave_df))
# wave_df.dropna( axis=0,inplace=True)
# print(len(wave_df))

wave_df.head()

data = load_wave()

def plotClasses(df):
    cols = ["col"+str(x) for x in range(1,len(df.columns))]
    cols.append("target")
    df.columns = cols # add column names to the data frame 
    counts = df.groupby(by="target").count()[['col1']]
    labels = counts.index.to_list()
    data_points = counts.values.reshape(len(counts),)
    return plt.bar(labels,data_points)

plotClasses(wave_df)

"""### Run conformal prediction

##### 1-svm
"""

svm_mean_df = analysis_with_2_func(data=data, model_obj=svm_model_obj, 
                     params=svm_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
svm_mean_df

"""##### 2-decision tree"""

dt_mean_df = analysis_with_2_func(data=data, model_obj=dt_model_obj, 
                     params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))}, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
dt_mean_df

"""##### 3 - KNeighbors"""

knn_mean_df = analysis_with_2_func(data=data, model_obj=k_kneighbors_model_obj, 
                     params=k_kneighbors_params, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
knn_mean_df

"""#####  4 - AdaBoostClassifier"""

ada_boost_mean_df = analysis_with_2_func(data=data, model_obj=ada_boost_model_obj, 
                     params=ada_boost_params, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
ada_boost_mean_df

"""##### 5 - GaussianNB"""

gussian_nb_mean_df = analysis_with_2_func(data=data,model_obj=gaussian_nb_model_obj, 
                     params=gaussian_nb_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
gussian_nb_mean_df

"""##### 6 - MLPClassifier"""

mlp_classifier_mean_df = analysis_with_2_func(data=data, model_obj=mlp_classifier_model_obj, 
                     params=mlp_classifier_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
mlp_classifier_mean_df

"""###### 7 - RandomForestClassifier"""

random_forest_params[MIN_SAMPLES_KEY] = max(5, int(dt_min_sample_ratio * len(data['data'])))
random_forest_params

random_forest_mean_df = analysis_with_2_func(data=data, model_obj=random_forest_model_obj, 
                     params=random_forest_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
random_forest_mean_df

"""###### 8 - GaussianProcessClassifier"""

gaussian_process_mean_df = pd.DataFrame()

gaussian_process_mean_df = analysis_with_2_func(data=data, model_obj=gaussian_process_obj, 
                     params=gaussian_process_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
gaussian_process_mean_df

"""##### 9 - QuadraticDiscriminantAnalysis"""

quadratic_discriminant_mean_df = analysis_with_2_func(data=data, model_obj=quadratic_discriminant_model_obj, 
                     params=quadratic_discriminant_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
quadratic_discriminant_mean_df

"""### Plot results """

# give names to dataFrames
svm_mean_df.name = 'SVM'
dt_mean_df.name = 'DecisionTree'
knn_mean_df.name = 'KNN'
ada_boost_mean_df.name = 'AdaBoost'
gussian_nb_mean_df.name = 'GaussianNB'
mlp_classifier_mean_df.name = 'Multilayer Perceptron'
random_forest_mean_df.name = 'Random Forest'
gaussian_process_mean_df.name = 'Gaussian process'
quadratic_discriminant_mean_df.name = 'Quadratic discriminant analysis'

all_results = [svm_mean_df, dt_mean_df, knn_mean_df, ada_boost_mean_df, 
               gussian_nb_mean_df, mlp_classifier_mean_df, random_forest_mean_df,
               gaussian_process_mean_df, quadratic_discriminant_mean_df
               ]

"""### Error rate"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=['origin_err', 'marg_err', 'inv_err', 'Simul_err'], 
          title_str='Error rate', to_extend=[0.3, 0.4, 0.5])

"""### oneC"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=[None, 'marg_oneC', 'inv_oneC', 'Simul_oneC'], 
          title_str='oneC')#, to_extend=[0.3, 0.4, 0.5])

"""### avgC"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=[None, 'marg_avgC', 'inv_avgC', 'Simul_avgC'], 
          title_str='avgC')#, to_extend=[0.3, 0.4, 0.5])

"""<font color='red'>Results: Which non-conformity function is better for which metric (oneC should be hight, avgC shold be low)</font>

|   |                        | avgC      | oneC      |
|---|---                     |---        |---        |
| 1 | SVM                    | inv_prob  | margin    |
| 2 | Decision trees         | inv_prob  | margin    |
| 3 | KNN                    | inv_prob  | margin    |
| 4 | AdaBoost               | -----     | ------    |
| 5 | GaussianNB             | inv_prob  | margin    |
| 6 | Multilayer Perceptron  | inv_prob  | margin    |
| 7 | Random Forest          | inv_prob  | margin    |
| 8 | Gaussian process       | inv_prob  | margin    |
| 9 | Quadratic discriminant analysis  | inv_prob  | margin    |

### svm

###MarginErrFunc
"""

res_df = run_conformal(data, model_obj=svm_model_obj, 
                       params=svm_params,
                       epsilon_arr=eps_err, err_func=MarginErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
svm_mean_df = res_df.groupby('eps').mean()
svm_mean_df

"""### InverseProbabilityErrFunc"""

res_df = run_conformal(data, epsilon_arr=eps_err, err_func=InverseProbabilityErrFunc, smoothing=True,
                             seed=1, verbose=True)
# Get average values of metrics
print('average values for each eps')
res_df.groupby('eps').mean()

tmp_df = res_df.groupby('eps').mean()
# rename columns for margin function
svm_mean_df.columns = ['fold', 'origin_err', 'marg_err', 'marg_oneC', 'marg_avgC', 
                       'Simul_err', 'Simul_oneC', 'Simul_avgC']
# these columns contain results for inverse probability non-conformity function
svm_mean_df['inv_err'] = tmp_df['Conf_err']
svm_mean_df['inv_oneC'] = tmp_df['Conf_oneC']
svm_mean_df['inv_avgC'] = tmp_df['Conf_avgC']
svm_mean_df

"""### decision tree

### MarginErrFunc
"""

res_df = run_conformal(data, model_obj=dt_model_obj, 
                       params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))},
                       epsilon_arr=eps_err, err_func=InverseProbabilityErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
res_df.groupby('eps').mean()



"""### InverseProbabilityErrFunc"""

res_df = run_conformal(data, model_obj=dt_model_obj, 
                       params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))},
                       epsilon_arr=eps_err, err_func=InverseProbabilityErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
res_df.groupby('eps').mean()

tmp_df = res_df.groupby('eps').mean()
# rename columns for margin function
dt_mean_df.columns = ['fold', 'origin_err', 'marg_err', 'marg_oneC', 'marg_avgC', 
                       'Simul_err', 'Simul_oneC', 'Simul_avgC','inv_err','inv_oneC','inv_avgC']
# these columns contain results for inverse probability non-conformity function
dt_mean_df['inv_err'] = tmp_df['Conf_err']
dt_mean_df['inv_oneC'] = tmp_df['Conf_oneC']
dt_mean_df['inv_avgC'] = tmp_df['Conf_avgC']
dt_mean_df



"""### Plot results: error rate"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['origin_err'], label='SVM', color=color, lw=3) # thik line for original error
plt.plot(eps_err, df['marg_err'], label='- margin', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_err'], label='- inv prob', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_err'], label='- simulated', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['origin_err'], label='Decision tree', color=color, lw=3) # thik line for original error
plt.plot(eps_err, df['marg_err'], label='_nolegend_', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_err'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_err'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
y_ticks = eps_err[:]
y_ticks.extend([0.3, 0.4, 0.5])
plt.yticks(y_ticks)
plt.grid(True)
plt.legend()
plt.title('Error rate')

"""### oneC"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['marg_oneC'], label='SVM', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_oneC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_oneC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['marg_oneC'], label='Decision tree', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_oneC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_oneC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
plt.grid(True)
plt.legend()
plt.title('oneC')



"""### avgC"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['marg_avgC'], label='SVM', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_avgC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_avgC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['marg_avgC'], label='Decision tree', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_avgC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_avgC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
plt.grid(True)
plt.legend()
plt.title('avgC')



"""### tai"""

from sklearn.preprocessing import LabelEncoder
def load_tai():
    print('Loading tai')
    # load tai dataset
    file = path_to_files+'datasets/TAI.csv'
    df = pd.read_csv(file,header=0, sep=',')
#     df.drop(0, axis=1,inplace=True)
    target_names = df.iloc[:,1].unique()
    target_names.sort()
    
    target = df.iloc[:,1].values 
    label_encoder = LabelEncoder()
    target = label_encoder.fit_transform(target)
    # scale target 
#     target -= min(target)

    data = {'target': target, 'data': df.iloc[:,2:-1].values,
            'target_names': target_names}
    return data

tai_df = pd.read_csv(path_to_files +'datasets/TAI.csv',header=0,sep=',')
print(tai_df.shape)
# balance_df.drop(0, axis=1,inplace=True)
tai_df.head()
# tai_df.iloc[:,1].unique()

data = load_tai()

def plotClasses(df):
    cols = ["col"+str(x) for x in range(1,len(df.columns))]
    cols.append("target")
    df.columns = cols # add column names to the data frame 
    counts = df.groupby(by="target").count()[['col1']]
    labels = counts.index.to_list()
    data_points = counts.values.reshape(len(counts),)
    return plt.bar(labels,data_points)

plotClasses(tai_df)

"""### Run conformal prediction

##### svm
"""

svm_mean_df = analysis_with_2_func(data=data, model_obj=svm_model_obj, 
                     params=svm_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
svm_mean_df

"""##### 2-decision tree"""

dt_mean_df = analysis_with_2_func(data=data, model_obj=dt_model_obj, 
                     params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))}, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
dt_mean_df

"""##### 3 - KNeighbors"""

knn_mean_df = analysis_with_2_func(data=data, model_obj=k_kneighbors_model_obj, 
                     params=k_kneighbors_params, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
knn_mean_df

"""##### 4 - AdaBoostClassifier"""

ada_boost_mean_df = analysis_with_2_func(data=data, model_obj=ada_boost_model_obj, 
                     params=ada_boost_params, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
ada_boost_mean_df

"""##### 5 - GaussianNB"""

ada_boost_mean_df = analysis_with_2_func(data=data, model_obj=ada_boost_model_obj, 
                     params=ada_boost_params, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
ada_boost_mean_df

"""##### 6 - MLPClassifier"""

mlp_classifier_mean_df = analysis_with_2_func(data=data, model_obj=mlp_classifier_model_obj, 
                     params=mlp_classifier_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
mlp_classifier_mean_df

"""##### 7 - RandomForestClassifier"""

random_forest_params[MIN_SAMPLES_KEY] = max(5, int(dt_min_sample_ratio * len(data['data'])))
random_forest_params

random_forest_mean_df = analysis_with_2_func(data=data, model_obj=random_forest_model_obj, 
                     params=random_forest_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
random_forest_mean_df

"""##### 8 - GaussianProcessClassifier"""

gaussian_process_mean_df = pd.DataFrame()

gaussian_process_mean_df = analysis_with_2_func(data=data, model_obj=gaussian_process_obj, 
                     params=gaussian_process_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
gaussian_process_mean_df

"""##### 9 - QuadraticDiscriminantAnalysis"""

quadratic_discriminant_mean_df = analysis_with_2_func(data=data, model_obj=quadratic_discriminant_model_obj, 
                     params=quadratic_discriminant_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
quadratic_discriminant_mean_df

"""### Plot results"""

# give names to dataFrames
svm_mean_df.name = 'SVM'
dt_mean_df.name = 'DecisionTree'
knn_mean_df.name = 'KNN'
ada_boost_mean_df.name = 'AdaBoost'
gussian_nb_mean_df.name = 'GaussianNB'
mlp_classifier_mean_df.name = 'Multilayer Perceptron'
random_forest_mean_df.name = 'Random Forest'
gaussian_process_mean_df.name = 'Gaussian process'
quadratic_discriminant_mean_df.name = 'Quadratic discriminant analysis'

all_results = [svm_mean_df, dt_mean_df, knn_mean_df, ada_boost_mean_df, 
               gussian_nb_mean_df, mlp_classifier_mean_df, random_forest_mean_df,
               gaussian_process_mean_df, quadratic_discriminant_mean_df
               ]

"""#### error rate"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=['origin_err', 'marg_err', 'inv_err', 'Simul_err'], 
          title_str='Error rate', to_extend=[0.3, 0.4, 0.5])

"""oneC"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=[None, 'marg_oneC', 'inv_oneC', 'Simul_oneC'], 
          title_str='oneC')#, to_extend=[0.3, 0.4, 0.5])

"""### avgC"""

k = 8
plot_2_df(df_1=svm_mean_df, df_2=all_results[k-1], 
          columns=[None, 'marg_avgC', 'inv_avgC', 'Simul_avgC'], 
          title_str='avgC')#, to_extend=[0.3, 0.4, 0.5])

"""<font color='red'>Results: Which non-conformity function is better for which metric (oneC should be hight, avgC shold be low)</font>

|   |                        | avgC      | oneC      |
|---|---                     |---        |---        |
| 1 | SVM                    | inv_prob  | margin    |
| 2 | Decision trees         | inv_prob  | margin    |
| 3 | KNN                    | inv_prob  | margin    |
| 4 | AdaBoost               | inv_prob     | margin   |
| 5 | GaussianNB             | inv_prob  | margin    |
| 6 | Multilayer Perceptron  | inv_prob  | margin    |
| 7 | Random Forest          | inv_prob  | margin    |
| 8 | Gaussian process       | inv_prob  | margin    |
| 9 | Quadratic discriminant analysis  | inv_prob  | margin    |

### svm

### MarginErrFunc
"""

res_df = run_conformal(data, epsilon_arr=eps_err, err_func=MarginErrFunc, smoothing=True, 
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
res_df.groupby('eps').mean()

"""### InverseProbabilityErrFunc"""

res_df = run_conformal(data, epsilon_arr=eps_err, err_func=InverseProbabilityErrFunc, smoothing=True,
                             seed=1, verbose=True)
# Get average values of metrics
print('average values for each eps')
res_df.groupby('eps').mean()

tmp_df = res_df.groupby('eps').mean()
# rename columns for margin function
svm_mean_df.columns = ['fold', 'origin_err', 'marg_err', 'marg_oneC', 'marg_avgC', 
                       'Simul_err', 'Simul_oneC', 'Simul_avgC','inv_err','inv_oneC','inv_avgC']
# these columns contain results for inverse probability non-conformity function
svm_mean_df['inv_err'] = tmp_df['Conf_err']
svm_mean_df['inv_oneC'] = tmp_df['Conf_oneC']
svm_mean_df['inv_avgC'] = tmp_df['Conf_avgC']
svm_mean_df

"""### decision tree

### MarginErrFunc
"""

res_df = run_conformal(data, epsilon_arr=eps_err, err_func=MarginErrFunc, smoothing=True,
                       seed=1, verbose=True)
# Get average values of metrics
print('\naverage values for each eps')
mean_df = res_df.groupby('eps').mean()
mean_df



"""### InverseProbabilityErrFunc"""

res_df = run_conformal(data, epsilon_arr=eps_err, err_func=InverseProbabilityErrFunc, smoothing=True,
                             seed=1, verbose=True)
# Get average values of metrics
print('average values for each eps')
res_df.groupby('eps').mean()

tmp_df = res_df.groupby('eps').mean()
# rename columns for margin function
dt_mean_df.columns = ['fold', 'origin_err', 'marg_err', 'marg_oneC', 'marg_avgC', 
                       'Simul_err', 'Simul_oneC', 'Simul_avgC','err_inv','inv_oneC','inv_avgC']
# these columns contain results for inverse probability non-conformity function
dt_mean_df['inv_err'] = tmp_df['Conf_err']
dt_mean_df['inv_oneC'] = tmp_df['Conf_oneC']
dt_mean_df['inv_avgC'] = tmp_df['Conf_avgC']
dt_mean_df



"""Plot results: error rate"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['origin_err'], label='SVM', color=color, lw=3) # thik line for original error
plt.plot(eps_err, df['marg_err'], label='- margin', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_err'], label='- inv prob', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_err'], label='- simulated', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['origin_err'], label='Decision tree', color=color, lw=3) # thik line for original error
plt.plot(eps_err, df['marg_err'], label='_nolegend_', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_err'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_err'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
y_ticks = eps_err[:]
y_ticks.extend([0.3, 0.4, 0.5])
plt.yticks(y_ticks)
plt.grid(True)
plt.legend()
plt.title('Error rate')

"""### oneC"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['marg_oneC'], label='SVM', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_oneC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_oneC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['marg_oneC'], label='Decision tree', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_oneC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_oneC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
plt.grid(True)
plt.legend()
plt.title('oneC')



"""### avgC"""

# SVM color - blue
color = 'blue'
df = svm_mean_df
plt.plot(eps_err, df['marg_avgC'], label='SVM', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_avgC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_avgC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

# Decision tree color - orange
color = 'orange'
df = dt_mean_df
plt.plot(eps_err, df['marg_avgC'], label='Decision tree', color=color, ) # normal line for margin error
plt.plot(eps_err, df['inv_avgC'], label='_nolegend_', color=color, linestyle='--') # dashed line for inv_prob error
plt.plot(eps_err, df['Simul_avgC'], label='_nolegend_', color=color, linestyle=':') # dotted line for simulation error

plt.xticks(eps_err)
plt.grid(True)
plt.legend()
plt.title('avgC')

"""## car"""

from sklearn.preprocessing import LabelEncoder
def load_cars():
    print('Loading cars')
    # load cars dataset
    file = path_to_files +'datasets/cars.csv'
    df = pd.read_csv(file,header=None, sep=',')
#     df.drop(0, axis=0,inplace=True)
    target_names = df[df.columns[-1]].unique()
    target_names.sort()
    data_points = df[df.columns[:-1]]
    target = df[df.columns[-1]].values 
    label_encoder = LabelEncoder()
   
    target = label_encoder.fit_transform(target)
    for i in range(6):
        data_points.iloc[:,i] = label_encoder.fit_transform(data_points.iloc[:,i])
    
#     data_points = data_points.apply(lambda x:label_encoder.fit_transform(x),axis=1)
  
    # scale target 
#     target -= min(target)
    data = {'target': target, 'data': data_points.values,
            'target_names': target_names}
    
    return data

cars_df = pd.read_csv(path_to_files +'datasets/cars.csv',header=None,sep=',')
# print(len(cars_df))
# balance_df.drop(0, axis=0,inplace=True)
# print(len(cars_df))
cars_df.head()

data = load_cars()

def plotClasses(df):
    cols = ["col"+str(x) for x in range(1,len(df.columns))]
    cols.append("target")
    df.columns = cols # add column names to the data frame 
    counts = df.groupby(by="target").count()[['col1']]
    labels = counts.index.to_list()
    data_points = counts.values.reshape(len(counts),)
    return plt.bar(labels,data_points)

plotClasses(cars_df)

"""### 1-svm"""

svm_mean_df = analysis_with_2_func(data=data, model_obj=svm_model_obj, 
                     params=svm_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
svm_mean_df

"""### 2- decision tree

"""

dt_mean_df = analysis_with_2_func(data=data, model_obj=dt_model_obj, 
                     params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))}, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
dt_mean_df

"""### 3- KNeighbors"""

knn_mean_df = analysis_with_2_func(data=data, model_obj=k_kneighbors_model_obj, 
                     params=k_kneighbors_params, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
knn_mean_df

"""### 4- AdaBoostClassifie"""

ada_boost_mean_df = analysis_with_2_func(data=data, model_obj=ada_boost_model_obj, 
                     params=ada_boost_params, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
ada_boost_mean_df

"""### 5-GaussianNB"""

ada_boost_mean_df = analysis_with_2_func(data=data, model_obj=ada_boost_model_obj, 
                     params=ada_boost_params, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
ada_boost_mean_df

"""### 6- MLPClassifier"""

mlp_classifier_mean_df = analysis_with_2_func(data=data, model_obj=mlp_classifier_model_obj, 
                     params=mlp_classifier_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
mlp_classifier_mean_df

"""### 7-RandomForestClassifier"""

random_forest_params[MIN_SAMPLES_KEY] = max(5, int(dt_min_sample_ratio * len(data['data'])))
random_forest_params

random_forest_mean_df = analysis_with_2_func(data=data, model_obj=random_forest_model_obj, 
                     params=random_forest_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
random_forest_mean_df

"""### 8-GaussianProcessClassifier"""

gaussian_process_mean_df = pd.DataFrame()

gaussian_process_mean_df = analysis_with_2_func(data=data, model_obj=gaussian_process_obj, 
                     params=gaussian_process_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
gaussian_process_mean_df

"""### 9- QuadraticDiscriminantAnalysis"""

quadratic_discriminant_mean_df = analysis_with_2_func(data=data, model_obj=quadratic_discriminant_model_obj, 
                     params=quadratic_discriminant_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
quadratic_discriminant_mean_df

"""## heat"""

from sklearn.preprocessing import LabelEncoder
def load_heat():
    print('Loading heat')
    # load heat dataset
    file = path_to_files +'datasets/heat.csv'
    df = pd.read_csv(file,header=0, sep=',')
    target_names = df.iloc[:,-1].unique()
    target_names.sort()
    label_encoder = LabelEncoder()
    data_points = df.iloc[:,2:-1] 
    target = df[df.columns[-1]].values 
    
    # scale target 
#     target -= min(target)
    data = {'target': target, 'data': data_points.values,
            'target_names': target_names}
    return data

heat_df = pd.read_csv(path_to_files + 'datasets/heat.csv',header=0,sep=',')
print(heat_df.shape)
# balance_df.drop(0, axis=1,inplace=True)
heat_df.head()
# heat_df.iloc[:,-1].unique()

data = load_heat()

def plotClasses(df):
    cols = ["col"+str(x) for x in range(1,len(df.columns))]
    cols.append("target")
    df.columns = cols # add column names to the data frame 
    counts = df.groupby(by="target").count()[['col1']]
    labels = counts.index.to_list()
    data_points = counts.values.reshape(len(counts),)
    return plt.bar(labels,data_points)

plotClasses(heat_df)

"""###1-svm"""

svm_mean_df = analysis_with_2_func(data=data, model_obj=svm_model_obj, 
                     params=svm_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
svm_mean_df

"""### 2- decision tree"""

dt_mean_df = analysis_with_2_func(data=data, model_obj=dt_model_obj, 
                     params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))}, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
dt_mean_df

"""### 3- KNeighbors"""

knn_mean_df = analysis_with_2_func(data=data, model_obj=k_kneighbors_model_obj, 
                     params=k_kneighbors_params, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
knn_mean_df

"""### 4- AdaBoostClassifie"""

ada_boost_mean_df = analysis_with_2_func(data=data, model_obj=ada_boost_model_obj, 
                     params=ada_boost_params, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
ada_boost_mean_df

"""### 5-GaussianNB"""

ada_boost_mean_df = analysis_with_2_func(data=data, model_obj=ada_boost_model_obj, 
                     params=ada_boost_params, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
ada_boost_mean_df

"""### 6- MLPClassifier"""

mlp_classifier_mean_df = analysis_with_2_func(data=data, model_obj=mlp_classifier_model_obj, 
                     params=mlp_classifier_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
mlp_classifier_mean_df

"""### 7-RandomForestClassifier"""

random_forest_params[MIN_SAMPLES_KEY] = max(5, int(dt_min_sample_ratio * len(data['data'])))
random_forest_params

random_forest_mean_df = analysis_with_2_func(data=data, model_obj=random_forest_model_obj, 
                     params=random_forest_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
random_forest_mean_df

"""### 8 - GaussianProcessClassifier"""

gaussian_process_mean_df = pd.DataFrame()

gaussian_process_mean_df = analysis_with_2_func(data=data, model_obj=gaussian_process_obj, 
                     params=gaussian_process_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
gaussian_process_mean_df

"""### 9-quadratic Discriminant"""

quadratic_discriminant_mean_df = analysis_with_2_func(data=data, model_obj=quadratic_discriminant_model_obj, 
                     params=quadratic_discriminant_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
quadratic_discriminant_mean_df

"""### wine"""

from sklearn.preprocessing import LabelEncoder
def load_wine():
    print('Loading wine')
    # load wne dataset
    file = path_to_files +'datasets/wine.csv'
    df = pd.read_csv(file,header=None, sep=',')
#     df.drop(0, axis=1,inplace=True)
    target_names = df.iloc[:,0].unique()
    target_names.sort()
    
    target = df.iloc[:,0].values 
    
    # scale target 
#     target -= min(target)
    data = {'target': target, 'data': df.iloc[:,1:-1].values,
            'target_names': target_names}
    return data

wine_df = pd.read_csv(path_to_files +'datasets/wine.csv',header=None,sep=',')
# print(len(wine_df))
# balance_df.drop(0, axis=1,inplace=True)
wine_df.head()
wine_df.iloc[:,0].unique()

data = load_wine()

def plotClasses(df):
    cols = ["col"+str(x) for x in range(1,len(df.columns))]
    cols.insert(0,"target")
    df.columns = cols # add column names to the data frame 
    counts = df.groupby(by="target").count()[['col1']]
    labels = counts.index.to_list()
    data_points = counts.values.reshape(len(counts),)
    return plt.bar(labels,data_points)

plotClasses(wine_df)

"""#### 1-svm"""

svm_mean_df = analysis_with_2_func(data=data, model_obj=svm_model_obj, 
                     params=svm_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
svm_mean_df

"""#### 2-decision tree"""

dt_mean_df = analysis_with_2_func(data=data, model_obj=dt_model_obj, 
                     params={MIN_SAMPLES_KEY: max(5, int(dt_min_sample_ratio * len(data['data'])))}, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
dt_mean_df

"""#### 3- KNeighbors"""

knn_mean_df = analysis_with_2_func(data=data, model_obj=k_kneighbors_model_obj, 
                     params=k_kneighbors_params, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
knn_mean_df

"""#### 4- AdaBoostClassifie"""

ada_boost_mean_df = analysis_with_2_func(data=data, model_obj=ada_boost_model_obj, 
                     params=ada_boost_params, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
ada_boost_mean_df

"""##### 5-GaussianNB"""

ada_boost_mean_df = analysis_with_2_func(data=data, model_obj=ada_boost_model_obj, 
                     params=ada_boost_params, 
                     epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
ada_boost_mean_df

"""#### 6 - MLPClassifier"""

mlp_classifier_mean_df = analysis_with_2_func(data=data, model_obj=mlp_classifier_model_obj, 
                     params=mlp_classifier_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
mlp_classifier_mean_df

"""##### 7- random forest"""

random_forest_params[MIN_SAMPLES_KEY] = max(5, int(dt_min_sample_ratio * len(data['data'])))
random_forest_params

random_forest_mean_df = analysis_with_2_func(data=data, model_obj=random_forest_model_obj, 
                     params=random_forest_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
random_forest_mean_df

"""#### 8 - GaussianProcessClassifier"""

gaussian_process_mean_df = pd.DataFrame()

gaussian_process_mean_df = analysis_with_2_func(data=data, model_obj=gaussian_process_obj, 
                     params=gaussian_process_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
gaussian_process_mean_df

"""#### 9-Quadratic Discriminant"""

quadratic_discriminant_mean_df = analysis_with_2_func(data=data, model_obj=quadratic_discriminant_model_obj, 
                     params=quadratic_discriminant_params, epsilon_arr=eps_err, 
                     smoothing=True, seed=1, sub_verbose=True)
quadratic_discriminant_mean_df

